# Multiplication and Inverse Matrices

[Youtube link](https://www.youtube.com/watch?v=FX4C-JpTFgY&t=1s)
[0. Introduction to Linear Algebra](0.%20Introduction%20to%20Linear%20Algebra.md)

## Five ways of looking at matrix multiplication

Let's do this with an example.

$$
\begin{aligned}
AX&=B \\
\begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix}\cdot\begin{bmatrix}2&4\\3&1\\1&5\end{bmatrix}&=\begin{bmatrix}11&21\\29&51\\47&81\end{bmatrix}
\end{aligned}
$$

### First way: dot product

$Element_{i,j}\ \mathrm{of\ B}$  = dot product of row i of $A$ and column j of $X$.
e.g., $11=1\cdot2+2\cdot3+3\cdot1$

### Second way: combination of columns

$Column_j \mathrm{\ of\ B}$ = linear combination of columns of $A$ with the elements of column j of $X$
e.g., $$\begin{bmatrix}11\\29\\47\end{bmatrix}=2\cdot\begin{bmatrix}1\\4\\7\end{bmatrix}+3\cdot\begin{bmatrix}2\\5\\8\end{bmatrix}+1\cdot\begin{bmatrix}3\\6\\9\end{bmatrix}$$
This is also how I think about the $Ax=b$ problems, where $x$ and $b$ are vectors.

### Third way: combination of rows

$Row_i \mathrm{\ of\ B}$ = linear combination of rows of $X$ with the elements of row i of $X$
e.g., $$\begin{bmatrix}11&21\end{bmatrix}=1\cdot\begin{bmatrix}2&4\end{bmatrix}+2\cdot\begin{bmatrix}3&1\end{bmatrix}+3\cdot\begin{bmatrix}1&5\end{bmatrix}$$
### Fourth way: summation of matrices

Let $B_n$ be the product of column j of $A$ and row i of $X$. Then $B=\sum B_n$. For this example, $B_n$ is a $3\cdot 2$ matrix.
e.g., $$
\begin{aligned}
B_1 &= \begin{bmatrix}1\\4\\7\end{bmatrix}\cdot\begin{bmatrix}2&4\end{bmatrix}\\
B_1 &= \begin{bmatrix}2&4\\8&16\\14&28\end{bmatrix}
\end{aligned}$$

Calculate $B_2$ and $B_3$ similarly. Sum them to get $B$.

Property for $B_n$: the row space and column space of $B_n$ is a line.

### Fifth way: block multiplication

Divide the matrices $A$ and $B$ into blocks. The $A_n$ and $B_n$ are blocks.

$$\begin{bmatrix}A_1&A_2\\A_3&A_4\end{bmatrix}\cdot\begin{bmatrix}B_1&B_2\\B_3&B_4\end{bmatrix}=\begin{bmatrix}C_1&C_2\\C_3&C_4\end{bmatrix}$$

Then $C_1 = A_1\cdot B_1 + A_2\cdot B_3$. It works just like numbers.

## Inverse matrices

### For a square matrix

$$A^{-1}A=I=AA^{-1}$$
$A$ is a square matrix, and $I$ is the identity matrix. 

If an inverse exists, the left inverse is also the right inverse.

> [!summary] Invertible Matrix
> An inverse exists if the matrix is not [singular](Singular%20Matrix.md). Then we call the matrix invertible.

### Square matrix with no inverse


Here is an intuitive explanation. Let's say we have a singular matrix: $$\begin{bmatrix}1&3\\2&6\end{bmatrix}$$
No matter how we combine its column and its rows, since they are on the same line, we cannot get a column in the identity matrix like $[1,0]$.

To summarize: a square matrix has no inverse if I can find a vector x with $Ax=0$. When x = $\begin{bmatrix}3\\-1\end{bmatrix}$, $Ax$ is 0.

### Compute inverses

$$\begin{bmatrix}1&3\\2&7\end{bmatrix}\cdot \begin{bmatrix}a&c\\b&d\end{bmatrix}=\begin{bmatrix}1&0\\0&1\end{bmatrix}$$

Solve a system of equations:
$$\begin{aligned}a\begin{bmatrix}1\\2\end{bmatrix}+b\begin{bmatrix}3\\7\end{bmatrix}&=\begin{bmatrix}1\\0\end{bmatrix}\\
c\begin{bmatrix}1\\2\end{bmatrix}+d\begin{bmatrix}3\\7\end{bmatrix}&=\begin{bmatrix}1\\0\end{bmatrix}
\end{aligned}$$

**Gauss-Jordan's way**:

$$\begin{aligned}
&\begin{bmatrix}1&3&1&0\\2&7&0&1\end{bmatrix}\\
&=\begin{bmatrix}1&3&1&0\\0&1&-2&1\end{bmatrix}\\
&=\begin{bmatrix}1&0&7&-3\\0&1&-2&1\end{bmatrix}
\end{aligned}$$
1. Concatenate $A$ and $I$
2. Do Gaussian elimination so $A$ becomes $I$
3. The right matrix becomes $A^{-1}$

Here is why:
$$\begin{bmatrix}A&I\end{bmatrix}E=\begin{bmatrix}I&A^{-1}\end{bmatrix}$$

- Remember that matrix multiplication is matrix manipulation. $E$ is all the eliminations done on the concatenated matrix. 
- If $E$ turns $A$ into $I$, then $E$ must be $A^{-1}$ because $AE=I$ is the definition of inverse matrix.
- Since multiplying with $I$ does not change the matrix. $IE=A^{-1}$.















