# Lecture 3

> [Youtube](https://www.youtube.com/watch?v=Xa2jPbURTjQ&list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k&index=6)
> [0. Linear Algebra for Machine Learning](0.%20Linear%20Algebra%20for%20Machine%20Learning.md)

## Matrix Q

- Q has [orthonormal columns](Orthonormal%20Vectors.md)
- $Q^TQ=I$
	- I: the identity matrix
	- A vector dot itself gives its [magnitude](Magnitude%20of%20a%20Vector.md) squared, which is 1 since the columns are unit vectors 
	- All other multiplications give 0 since they are [orthonormal](Orthonormal%20Vectors.md)
- $QQ^T=I?$
	- True when Q is square, and Q is called an [orthogonal Matrix](Orthogonal%20Matrix.md)
	- False when Q is not square

## Rotation Matrix

$$
Q=\begin{bmatrix}  
cos(\theta) & -sin(\theta) \\  
sin(\theta) & cos(\theta)   
\end{bmatrix}
$$
- $Q$ is an orthonormal matrix
- $sin^2(\theta)+cos^2(\theta)=1$
- This matrix rotates a vector $x$ by $\theta$ degrees and does not change its length
	- $\left\Vert Qx \right\Vert = \left\Vert x \right\Vert$ for any x
	- No **overflow** (getting extremely high numbers) can happen with multiplications of orthogonal matrices
- For a square orthogonal matrix, $Q^{T}= Q^{-1}$  

## Reflection Matrix

$$
Q=\begin{bmatrix}  
cos(\theta) & sin(\theta) \\  
sin(\theta) & -cos(\theta)   
\end{bmatrix}
$$
- [Determinant](Determinant.md) is -1
- Reflects a vector across the line $\theta/2$

> [!summary] Property
> 
> - Rotation times rotation = rotation. 
> - Reflection times reflection = rotation. 
> - Rotation times reflection = reflection. 
> - All still true in $R^n$.
## Householder Reflections

- Start with a unit vector $u^Tu=I$
- The Householder matrix
$$
H = I - 2uu^T
$$
- H is orthogonal and symmetric

## Hadamard Matrix

$$
H_{2}=\begin{bmatrix}  
1 & 1 \\  
1 & -1  
\end{bmatrix}
$$

$$
H_4=\begin{bmatrix}  
1 & 1 & 1 & 1\\  
1 & -1 & 1 & -1 \\
1 & 1 & -1 & -1 \\
1 & -1 & -1 & 1 
\end{bmatrix} = \begin{bmatrix}  
H_2 & H_2 \\  
H_2 & -H_2  
\end{bmatrix}
$$

$$
H_{8}=\begin{bmatrix}  
H_4 & H_4 \\  
H_4 & -H_4  
\end{bmatrix}
$$

- The columns are orthogonal
- Divide each matrix by magnitude of columns to get orthogonal matrices: $\sqrt2, \sqrt4, \sqrt8$ ...
- There is $H_{12}$

> [!summary] Hadamard Conjecture
> 
> There is a Hadamard matrix whenever N/4 is a whole number, up to 668. 

## Wavelets Matrices

$$
W_4 = \begin{bmatrix}  
1 & 1 & 1 & 0\\  
1 & 1 & -1 & 0\\
1 & -1 & 0 & 1\\
1 & -1 & 0 & -1
\end{bmatrix}
$$

$$
W_8 = \begin{bmatrix}  
1 & 1 & 1 & 0 & 1 & 0 & 0 & 0\\  
1 & 1 & 1 & 0 & -1 & 0 & 0 & 0\\
1 & 1 & -1 & 0 & 0 & 1 & 0 & 0\\
1 & 1 & -1 & 0 & 0 & -1 & 0 & 0\\
1 & -1 & 0 & 1 & 0 & 0 & 1 & 0\\  
1 & -1 & 0 & 1 & 0 & 0 & -1 & 0\\
1 & -1 & 0 & -1 & 0 & 0 & 0 & 1\\
1 & -1 & 0 & -1 & 0 & 0 & 0 & -1\\
\end{bmatrix}
$$

- Harr wavelets
- Wavelets are self scaling
- $W_4$
	- Column 1: take average
	- Column 2: taking difference
	- Column 3 and 4: taking difference at a smaller scale

## Orthogonal Eigenvectors

- Eigenvectors of a symmetric matrix are [orthogonal](Orthogonal%20Vectors.md)
- Eigenvectors of an [orthogonal matrix](Orthogonal%20Matrix.md) are [orthogonal](Orthogonal%20Vectors.md)
- Orthogonal vectors can be found easily this way

### Fourier Transform

- Discrete Fourier series uses orthogonal eigenvectors of Q:

$$
Q = \begin{bmatrix}  
0 & 1 & 0 & 0\\  
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0
\end{bmatrix}
$$
- Q: a permutation matrix, which is an orthogonal matrix
	- Eigenvectors of Q are the four discrete Fourier transforms
	- The eigenvectors are orthogonal and tremendously useful for signal processing

### Eigenvector Matrix of Q

$$
F_4 = \begin{bmatrix}  
1 & 1 & 1 & 1\\  
1 & i & i^2 & i^3\\
1 & i^2 & i^4 & i^6\\
1 & i^3 & i^6 & i^9
\end{bmatrix}
$$
- 1st column: zero frequency Fourier vector


> [!summary] Orthogonal Eigenvectors
> If 
> $$
> \begin{aligned}
> Q^TQ=I\\
> Qx=\lambda x\\
> Qy=\mu y
> \end{aligned}
> $$
> Then
> $$
> \overline x^Ty=0
> $$
> $\overline x$ denotes the complex conjugate of $x$

