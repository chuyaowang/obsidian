# HMM

[4. MIT CompBio - HMM](4.%20MIT%20CompBio%20-%20HMM.md)
[Slides](https://stellar.mit.edu/S/course/6/fa20/6.047/courseMaterial/topics/topic2/lectureNotes/Lecture04_HMMsI/Lecture04_HMMsI_II.pdf)

## Introduction

### Goals of the Lecture

- Introduce Hidden Markov Models (HMMs) and their relevance to biological sequence analysis, particularly in the context of probabilistic modeling.
- Provide an understanding of Bayesian inference and Bayes' Rule as foundational tools for reasoning about hidden states based on observed data.
- Explore the integration of Bayesian concepts with temporal dependencies through Markov chains.
- Illustrate the application of HMMs in genomic analysis with examples like CpG-rich promoter regions versus background sequences.
- Examine algorithms for decoding, scoring, and learning in HMMs to analyze biological data effectively.
- Lay the groundwork for advanced HMM applications, including comparative genomics, functional annotation, and whole-genome assembly.

## Review of Previous Lectures

### Tools and Concepts

- Dynamic Programming: 
	- Fundamental for solving sequence alignment problems and efficiently analyzing biological data.
	- Enables scoring and decoding paths in HMMs by storing intermediate computations.
- Hashing and String Search:
	- Techniques for rapid sequence matching and comparison.
	- Essential for applications like genome assembly and large-scale alignment.

### Biological Applications

What to do for a new sequence

- Sequence Alignment:
	- Align unknown sequences to known references to infer functional annotations or evolutionary relationships.
- Pattern Analysis:
	- Identify unique nucleotide compositions, motifs, or repeats that signify functional elements.
	- Basically stare at it and look for interesting features, such as non-standard nucleotide compositions, interesting k-mer frequencies, and recurrent patterns
- Genome Assembly:
	- Reconstruct complete genomes from fragmented sequence data, a crucial step in de novo sequencing projects.
- Today's topic: model it:
	- Make some hypotheses about it
	- Build a _generative_ model to describe it
	- Find sequences of similar _type_ (not identical but from the same type of functional element of DNA)

## Biological Sequence Modeling with HMMs

### Biological Context

- Biological sequences contain distinct functional elements
- Example: 
	- CpG islands, which are enriched in CG dinucleotides, often mark regulatory regions like promoters and can be identified using HMMs.
	- Introns
	- Promoters
- What to do with a big unlabeled region of DNA?
	-  Functional elements can be modeled probabilistically using HMMs.

### HMM modeling

- Ability to **emit** DNA sequences of a certain _type_
	- Not exact alignment to previously known gene
	- Preserving _properties_ of type, not identical sequence
- Ability to **recognize** DNA sequences of a certain type (**state**)
	- What hidden state is most likely given the observed sequence
	- Find set of states and transitions that generated a long sequence
- Ability to **learn** the distinguishing characteristics of each state
	- Train our generative model on large datasets
	- Learn to _classify_ unlabeled data
	- Example: learn from lots of promoters what a promoter looks like and generate more promoter sequences

### Applications

  - Detecting functional genomic regions such as promoters or CpG islands.
  - Generating synthetic sequences that preserve the statistical properties of biological data.
  - Modeling temporal or spatial dependencies in biological systems.

### Components of HMMs for Genomics

#### States

- Represent functional or structural categories within the genome, such as:
  - Promoters
  - Introns
  - Exons
  - Conserved regions
  - Background regions

#### Emission Probabilities

- Define the likelihood of observing specific nucleotides or k-mers in a given state.
  - Example: Promoters may have a higher likelihood of GC-rich k-mers, while background regions exhibit uniform nucleotide distributions.

#### Transition Probabilities

- Govern the likelihood of switching between states.
  - Example: Transitioning from a background region to a promoter region.
  - Reflect biological constraints such as the expected lengths of functional regions.

## Making inferences about the world

- We gather observations, but never the true state of the world that generates our observation. The true state is hidden from us.
- Generative models: express **forward** probability of an event, given the **hidden** state of the world
	- sampling of new data points by understanding the underlying generative process
	- crucial for biological sequence analysis as it helps uncover patterns that characterize functional elements such as promoters or coding regions.
- A generative model allows estimate $P(Snow | Winter)$, P(observation | hidden state) 
- Bayesian inference allows moving back the direction, P(hidden state | observation), P(Hypothesis | Data)

## Bayesian Inference and Bayes' Rule

### How it works

- Bayesian inference provides a mathematical framework for updating beliefs about hidden states based on observed data.
- Goal: P(D|H) -> P(H|D)
- Formula: Posterior probability = (Likelihood × Prior) / Evidence.
  - **Posterior**: The updated probability of a hypothesis given observed data. P(H|D)
  - **Likelihood**: P(D|H)
  - **Prior**: The initial belief or probability assigned to a hypothesis before observing data. P(H)
  - **Evidence (Marginal)**: The total probability of the observed data under all hypotheses, acting as a normalization factor.

### Proof

see [Bayes rule](1.%20VU%20Stat%20Probability%20Theory.md#Bayes%20rule)

### Example

$$
\begin{aligned}
Posterior &= \frac{Likelihood\times Prior}{Evidence} \\
P(H|D) &= \frac{P(D|H)P(H)}{P(D)}
\end{aligned}
$$

- Inferring the season based on observations like snow, sunshine, or rain. Snow suggests a higher probability of winter, while rain suggests summer.
- Posterior: probability of hypothesis being true after seen the data
	- probability of summer after seeing rain outside
- Prior: probability of hypothesis being true before seeing the data
	- probability of summer without knowing it's raining outside
- Evidence: Probability of collecting this data in all circumstances
	- probability of raining in all seasons
- Likelihood: Probability of collecting this data when our hypotheses is true
	- probability of seeing rain in summer

### Why probabilistic sequence modeling

- Biological data is noisy
- Probability provides a calculus for manipulating models
- Not limited to yes/no answers – can provide “degrees of belief”
- Many common computational tools based on probabilistic models
- Our tools:Markov Chains and Hidden Markov Models (HMMs)

## Markov Chains and Hidden Markov Models

### Data

- We don't have observation in isolation but a series of observations.
- The sequence contains more information for us to infer the hidden state
	- If we have weather observations for many days, it improves our posterior probability of being in a season given this series of observations.
	- Many days of snow gives us higher confidence it's winter compared to one day of snow
- HMM: couples Bayesian inference, hidden vs. observed state of the world with _a series of dependencies and transitions between your states_, governed by the Markov chain.

### Markov Chains

- A Markov chain is a stochastic model that describes _transitions between states_ in a sequence, assuming that the probability of moving to the next state depends only on the current state (**memorylessness**).
- Example: Seasonal transitions, where the probability of transitioning from winter to spring depends only on the current season (winter).
- Transition probabilities are represented in a matrix where each entry indicates the likelihood of moving from one state to another.

### Hidden Markov Models (HMMs)

- Hidden Markov Models (HMMs) extend Markov chains with a set of emission probabilities
	- We can decouple the the hidden state where the transition happens with the observations where the data is gathered.
	- The hidden state determines the emission probabilities
	- The hidden state transitions are governed by transition probabilities
- Compared to modeling weather transitions using HMM, a Markov chain of weather transitions has no memory of the past. The next state depends only on the current state. There is no hidden set of seasons that govern them.

### Components of HMM (formally)

  - Hidden states: Represent latent processes or categories (e.g., promoter, background).
  - Emission probabilities: Define the likelihood of observing specific outputs (e.g., nucleotides) given a state.
  - Transition probabilities: Indicate the likelihood of transitioning between hidden states.

### HMM nomenclature

- Vector $x$: sequence of observations
- Vector $\pi$: sequence of hidden states
- Transition _matrix_ $A = a_{kl}$ where $a_{kl} = P(\pi_i=l|\pi_{i-1}=k)$. 
	- The transition probability to state $l$ given the last state was $k$.
- Emission _vector_ $E=e_k(x_i)$, probability of observing $x_i$ from state $k$.
	- a vector for each state
	- for all states it is a emission matrix
- Bayes rule: use $P(x_i|\pi_i=k)$ to estimate $P(\pi_i=k|x_i)$
	- The generative model: $P(x_i|\pi_i=k)$; generates observation given state.

## Example: Detecting GC-rich regions

### Motivation

- Functional genomic regions has characteristic features. For example, the human promoter has a high GC enrichment, high CpG enrichment, being in NFI region, etc.
	- Varies by species
- Model the probability of being in promoter region given the features like GC content
- This allows us to determine where is the TSS given a raw DNA sequence

### Parameters of model

- We model the sequence as having 2 states:
	- Promoter (P)
	- Background (B)
- Model different nucleotide compositions
	- These are the **emission probabilities** of the state
	- Background: 25% for A, T, C, and G
		- Given the state is background, emission probability the same each letter in the same frequency
	- Promoters: 80% C and G, 10% A and T
		- Given the state is promoter, higher emission probability for C and G
- This is a generative model, P(nucleotide|state), the **emit** part
	- ex. P(A|promoter)
- Then reverse probabilities using Bayes rule, the **recognize** part
- Also model length distribution with :
	- Promoter: 20 bp
	- Non-promoter: 100 bp
	- modeled with **transition probabilities** of the model
		- Background becomes promoter 1 out of 100 times = 0.01
		- Promoter becomes background 1 out of 20 times = 0.05
- Getting the transition and emit probabilities from sequencing data is the **learn** part

### HMM architecture

![](Pasted%20image%2020241213073313.png)



## Applications in Genome Annotation

### Promoter Detection

- Goal: Identify regions enriched in GC content that signify promoters, which regulate gene expression.
- HMM Setup:
	- Transitions: between 2 states with different nucleotide composition
	- Hidden state: GC-rich/AT-rich
	- Emissions: nucleotides

### Conservation Analysis

- Goal: Identify regions conserved across species, indicating functional constraints.
- HMM Setup:
	- Transitions: between 2 states with different conservation levels
	- Hidden state: conserved/non-conserved
	- Emissions: Conservation scores derived from cross-species sequence alignments

### Protein-Coding Exons

- Goal: Annotate exons, and noncoding region (introns, and untranslated regions)
- HMM Setup:
	- Transitions: between 2 states with different tri-nucleotide composition
	- Hidden state: coding/non-coding
	- Emissions: triplets of nucleotides

### More

- coming in future lectures

![](Pasted%20image%2020241213074738.png)

## The 6 algorithms for HMM Modeling

![](Pasted%20image%2020241213080927.png)

- 3 rows: 
	- emitting: given path what is the likelihood of having this observation (scoring)
	- recognizing: what is the maximum likelihood path given observation (decoding)
	- learning: getting the probabilities
- 2 columns: 1 path or all paths
---




## Algorithms for HMMs

### Goals

- Scoring:
  - Calculate the joint probability of an observed sequence and a corresponding hidden state path.
- Decoding:
  - Identify the most likely sequence of hidden states given observed data.
- Learning:
  - Optimize emission and transition probabilities using training datasets, enabling model refinement.

### Dynamic Programming in HMMs

- Overcomes the computational complexity of enumerating all possible paths by storing intermediate results.
- Ensures efficient evaluation of probabilities across sequences of varying lengths.

### Viterbi Algorithm

- Finds the most likely sequence of hidden states (path) for a given set of observations.
- Iteratively calculates probabilities for each state at each position in the sequence.
- Backtracking identifies the optimal path, ensuring accurate decoding.

