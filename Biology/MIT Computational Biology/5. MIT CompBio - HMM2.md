# HMM 2

[4. MIT CompBio - HMM](4.%20MIT%20CompBio%20-%20HMM.md)
[0. MIT CompBio Manolis Kellis](0.%20MIT%20CompBio%20Manolis%20Kellis.md)
[Slides](https://stellar.mit.edu/S/course/6/fa20/6.047/courseMaterial/topics/topic2/lectureNotes/Lecture04_HMMsI/Lecture04_HMMsI_II.pdf)
## Recap of the First Half

### Key Concepts Reviewed

- **HMM Basics**:
  - Explored evaluation, parsing, and posterior decoding.
  - Focused on separating hidden states (e.g., promoter/background) from observed emissions (e.g., DNA sequences).
  - Used Bayesian inference to reverse the directionality of probabilities for hypothesis testing.

- **Core Tasks in HMMs**:
  - **Emission**: Generating sequences of specific types (e.g., promoters).
  - **Recognition**: Identifying the most likely hidden states responsible for observed sequences.
  - **Learning**: Inferring the probability distributions for emissions and transitions.

- **Efficient Algorithms**:
  - Defined the **Viterbi Algorithm** for decoding the most likely state path through sequences.
  - Introduced the **Forward Algorithm** for calculating the total probability by summing over all paths.
  - Emphasized dynamic programmingâ€™s role in reducing computational complexity.

## Expanding the State Space

### Memory Constraints in HMMs

- HMMs are **memoryless**, meaning future states depend only on the current state.
- To incorporate additional memory:
  - **Increase the number of states** to encode prior context (e.g., the previous nucleotide).
  - Example: A two-state model for GC-rich vs. AT-rich regions expands to an eight-state model to account for dinucleotide frequencies.

### Application to CpG Islands

- **What are CpG Islands?**:
  - Regions enriched in CG dinucleotides, often located near promoters.
  - Important due to their evolutionary conservation and roles in gene regulation.

- **Model Design**:
  - States encode both the nucleotide composition (e.g., GC-rich) and dinucleotide context (e.g., CpG frequency).
  - Transition probabilities capture differences between CpG-rich and CpG-poor regions.

### Biological Significance of CpG Islands

- **Methylation Impact**:
  - CpG methylation often leads to deamination and conversion to TpG or CpA.
  - CpG islands are typically unmethylated in promoter regions, distinguishing them from the heavily methylated genome.
  - Identifying CpG islands highlights promoter regions and unmethylated, transcriptionally active DNA.

## Protein-Coding Gene Structure

### Overview of Gene Architecture

- Genes consist of coding (exons) and non-coding (introns) regions.
- Functional elements:
  - **Start codon** (ATG): Signals the beginning of translation.
  - **Stop codons** (TGA, TAA, TAG): Indicate the end of translation.
  - **Splice sites**: Donor (GT) and acceptor (AG) sequences mark exon-intron boundaries.

### Modeling Gene Structure with HMMs

- **State Representation**:
  - Include states for untranslated regions (UTRs), coding exons, introns, splice sites, and stop codons.
  - Example: Transitions from an intron to an exon must go through a splice site.

- **Reading Frame Considerations**:
  - The reading frame determines codon interpretation.
  - States track the reading frame (e.g., E0, E1, E2) to ensure proper codon alignment across exon transitions.

- **Strand-Specific Modeling**:
  - Both forward and reverse DNA strands are modeled simultaneously to capture all possible gene orientations.

## Posterior Decoding

### Concept

- **Posterior Decoding**:
  - Determines the most likely state at each position by summing probabilities over all paths.
  - Offers a more refined interpretation compared to Viterbi decoding, which focuses on a single most likely path.

### Comparison with Viterbi Decoding

- **Viterbi Decoding**:
  - Yields a single, valid sequence of states.
  - Ensures that transitions follow HMM rules.

- **Posterior Decoding**:
  - Identifies the state with the highest probability at each position.
  - May produce an invalid sequence of states (e.g., skipping required transitions).

### Algorithms for Posterior Decoding

- Combine the **Forward** and **Backward Algorithms**:
  - Forward algorithm computes probabilities from left to right.
  - Backward algorithm computes probabilities from right to left.
  - Posterior probability at each position combines these values.

- **Applications**:
  - Useful for classification and identifying uncertain regions in the sequence.

## Learning in HMMs

### Supervised Learning

- **Training with Labeled Data**:
  - Annotated sequences are used to estimate emission and transition probabilities.
  - Example:
    - For a state emitting nucleotide G 2 out of 5 times, the emission probability is 2/5.
    - Transition probabilities are calculated by counting state transitions in the labeled data.

- **Avoiding Overfitting**:
  - Use pseudocounts to ensure no probability is zero, which avoids infinite penalties in log-space calculations.

### Unsupervised Learning

- **Goal**: Infer both the annotation and parameters without labeled data.

- **Viterbi Training**:
  - Iteratively estimates parameters based on the most likely path.
  - Steps:
    1. Start with random parameters.
    2. Decode the sequence using the Viterbi algorithm.
    3. Update parameters based on the decoded path.
    4. Repeat until convergence.

- **Expectation-Maximization (Baum-Welch Algorithm)**:
  - Considers all possible paths, weighting each by its probability.
  - Steps:
    1. **E-step**: Estimate the expected probability of hidden states given the current parameters and sequence.
    2. **M-step**: Update parameters to maximize likelihood based on these expectations.
  - Advantages:
    - Captures uncertainty in state assignments.
    - Produces robust parameter estimates.

## Applications of HMMs in Genomics

### Gene Prediction

- Detects complex gene architectures, including:
  - Exons, introns, UTRs, and splice sites.
  - Accounts for reading frames and strand orientation.

### Chromatin State Annotation

- Models chromatin states using histone modification data.
  - Example: ChromHMM assigns states (e.g., enhancers, promoters) based on chromatin marks.

### Comparative Genomics

- Identifies conserved regions by modeling evolutionary constraints on sequences.

### Epigenomic Analysis

- Integrates DNA methylation, histone modifications, and chromatin accessibility to infer regulatory regions.

## Key Takeaways

### Insights

- HMMs provide a flexible and probabilistic framework for modeling diverse biological data.
- Posterior decoding complements Viterbi decoding for fine-grained sequence interpretation.
- Dynamic programming enables efficient computation for large datasets.

### Applications

- From gene prediction to chromatin annotation, HMMs address a wide range of genomic challenges.
- Unsupervised learning techniques expand HMM applications to unlabeled datasets.

### Future Directions

- Hybrid models combining HMMs with machine learning techniques, such as deep learning, promise enhanced predictive capabilities.
- Expanding state complexity and integrating multi-omics data will further refine biological insights.

