# Hashing, Database Search, BLAST algorithm

> [Course video](https://www.youtube.com/watch?v=pO3GCbmfUKQ&list=PLypiXJdtIca6dEYlNoZJwBaz__CdsaoKJ&index=3)
> [Slides](https://stellar.mit.edu/S/course/6/fa20/6.047/courseMaterial/topics/topic2/lectureNotes/Lecture03_SequenceAlignment/Lecture03_SequenceAlignment.pdf)
> [0. MIT CompBio Manolis Kellis](0.%20MIT%20CompBio%20Manolis%20Kellis.md)

## Global Alignment (Review)

The Needleman-Wunsch Algorithm

**Initialization**: $F(0,0)=0$
- Top left

**Iteration**: $$F(i,j)=max\quad\begin{cases}&F(i-1,j)-d\\&F(i,j-1)-d\\&F(i-1,j-1)+s(x_i,y_j)\end{cases}$$

**Termination**: Bottom right
## Local Alignment

The Smith-Waterman Algorithm

Problem statement: a local alignment between a string S and a string T is an alignment of a substring of S and a substring of T.

Why do local alignment:
- A gene coding for a protein may only have a portion of it that is conserved.
- Searching for a small gene in a large chromosome.
- Large segments often undergo rearrangements. Local alignment finds the regions that are conserved but in different positions.

How:
- Instead of finding a single long path, find multiple short paths that correspond to different matching pairs.
![](Pasted%20image%2020240519161544.png)

Important question: how to define an optimal local alignment? Do we include weak matches flanking the strong matches?

### Smith-Waterman Algorithm

The local alignment algorithm.

**Initialization**: $F(0,j)=F(i,0)=0$. 
- The first column and top row are all 0s
- This allows initialization from anywhere. In contrast to global alignment where $F(0,0)=0$, allowing initialization from the top left only.

**Iteration**: $$F(i,j)=max\quad\begin{cases}&0\\&F(i-1,j)-d\\&F(i,j-1)-d\\&F(i-1,j-1)+s(x_i,y_j)\end{cases}$$Having 0 allows restarting anywhere.

**Termination**: Anywhere

### Semi-global alignment

Might have rearrangement or non-sensical sequences in one of the sequences. You want to align with at least one sequence all the way to the end.

**Initialization**: $F(0,j)=F(i,0)=0$
- Start from first row or first column

**Iteration**: $$F(i,j)=max\quad\begin{cases}&F(i-1,j)-d\\&F(i,j-1)-d\\&F(i-1,j-1)+s(x_i,y_j)\end{cases}$$
- Same as that for global alignment. No restarting anywhere.

**Termination**: Bottom row or right column
- Must go all the way to the end for one of the sequences being aligned.

## Applying different gap penalty functions

Using a single score for all gaps is easy, but does not necessarily reflect the biological truth.

### A general gap penalty function F(gap_length)

**Initialization**: same

**Iteration**:
$$\mathrm{F(i,j)}\quad=\max\begin{cases}\mathrm{F(i-1,j-1)+s(x_i,y_j)}\\\max_{k=0\ldots i-1}\mathrm{F(k,j)-\gamma(i-k)}\\\max_{k=0\ldots j-1}\mathrm{F(i,k)-\gamma(j-k)}\end{cases}$$
**Termination**: same

The new iteration scheme allows any number of gaps. Instead of the immediate left and top cell of the current cell in the score matrix, it:
1. searches for all the cells on the top and on the left
2. add a gap penalty for each number of gaps  
3. take the max of the highest score from the top, the diagonal, and the left

This adds computational complexity, turning local computation (order of 1) to order of $n$ computations.
Do not need to be so general, counting all the top and left cells.
- Need some smarter ways to estimate what gaps are possible.

### Other gap penalty functions

#### Linear gap penalty

- $w(k)=k\cdot p$
- Same penalty for all gaps
- State: current index tells if in a gap or not
- Achievable using quadratic algorithm (even with linear space)
- What we have been using previously.

#### Quadratic

- $w(k)=p+q\cdot k+rk^2$
- State: needs to encode the length of the gap, which can be O(n). Record a gap number for each position.
- To encode it we need O($\log(n)$) bits of information. Not feasible.

#### Affine gap penalty

- When a polymerase makes a mistake and create gaps, it usually skips a few nucleotides in a row.
- Trying to model this behavior, we set an initial high cost (call it $\gamma_i$) for starting a gap, then incremental cost (call it $\gamma_C$) for continuing the gap.
	- $w(k)=p+q\cdot k$, where $q<p$.
	- $p$ is the initial cost.
	- $q$ is the incremental cost.
- So for each cell in our matrix, we have two scenarios:
	- It is starting a gap
	- It is continuing a gap
- We need a _state_ for each sequence: starting a gap or not. State refers to a state in HMM, next lecture.
- We need a second matrix in addition to our original score matrix to record both scenarios
	- If we start a gap, we take $\max(\mathrm{top}-\gamma_{init},\mathrm{left}-\gamma_{init},\mathrm{diagonal}+s(x_i+y_j))$
	- If we continue a gap, we calculate the left and top scores in our second matrix using $\gamma_i$. Then take $\max(\mathrm{top}-\gamma_{continue},\mathrm{left}-\gamma_{continue},\mathrm{diagonals}+s(x_i+y_j))$

####  Length (mod 3) gap penalty

- Gaps with lengths divisible by 3 are more likely (penalized less), since they do not cause frame shifts.
- This is feasible, but requires more _states_.
- Possible states are: starting, mod 3 = 1, mod 3 = 2, mod 3 = 0
- Mod refers to modulus, or remainder?

## Linear time pattern matching

### Search for a small gene in a long chromosome with no gaps

- Doing local alignment of the small gene with an entire chromosome
	- This is extremely expensive! 
	- Searching for 1000 bases in a million bases require 1 billion compares between characters.
- Alternative: **Karp-Rabin** algorithm

**Linear**: proportional to the time it takes to read in the data. A string of length n takes O(n) complexity to read. An algorithm that is O(n) is proportional to the reading time. It can be 3n, 4n, 6n, etc.

#### Karp-Rabin algorithm (probabilistic linear time)

- Interpret the string numerically
- Start with 'broken' version of the algorithm
- Progressively fix it to make it work

#### Deterministic linear time solutions

- Z-algorithm
- Boyer-Moore and Knuth-Morris-Pratt algorithms are earliest instantiations, similar in spirit
- Suffix trees: many different variations and applications; limited use in CompBio.
- Suffix arrays: practical variation, Gene Myers

## Karp-Rabin Algorithm

**Key idea**: Interpret string as a number. Instead of searching strings, you search for a number in a long sequence of numbers. So instead of comparing characters, you check if the numbers are equal.

Example:
T = `2359023141526739921`
P = `31415`

Moving along T, you compare if `23590 == 31415` then if `35902==31415` so on and so forth.

**Problem**: it takes linear time to compute the numbers from the numerals. To get 31415, need to do `3*10000+1*1000+4*100+1*10+5*1`.

**Solution**: compute the next number based on the previous one -> O(1)
- next_number = f(previous_number)
- easy to do
**Also Solution** : hashing (mod p) to keep the numbers small -> O(1)

### Hashing

**Problem**: The comparisons also take time. To get O(n) time, each comparison should be done in O(1) time (n comparisons between the pattern and the sequence). However, if the arguments are m-bits long ($2^m$ range, or $2^m$ possible numbers), each comparison takes O(m) time. If the argument is longer than 32 or 64 bits, they also need to be computed separately because the computer can only fit 32 bits or 62 bits at a time. The length of numbers must be reduced to something more manageable.

- Hashing maps keys $\textbf{k}$ from a large universe $\textbf{U}$ into the hash of each key $h(k)$ in a smaller space $[1\ldots m]$.
	- Here we are hashing numbers in the $2^m$ universe into a smaller space
- There are many ways possible
	- Summing all the bits
	- Take the modulus of a number
	- Take the first 20 bits or last 20 bits
- **Desired properties** of a hash function
	- Reproducibility: $x=y\Rightarrow h(x)=h(y)$ (hash of x always the same)
	- Uniform output distribution: output the hash function should uniformly span the entire smaller space
		- $x\ne y\Rightarrow P(h(x)=h(y))=\frac{1}{m}$  
		- minimizes chance of collision, which is two numbers having the same hash

**Strategy** for ensuring uniform output distribution: use all the bits. If we only use the first 2 bits, then `14123` and `14566` will have the same hash.
- mod 13: the remainder of dividing the number by 13. This operation uses all the bits.
- The modulus operation can be performed while computing the new number from the old number.
- `31415` corresponds to 7, `14152` corresponds to 8, for example.

### Collisions in hashing

- Hashing enables faster computation, but leads to spurious hits (collisions).
- When $h(x) = h(y)$ but $x \ne y$.
- Happens because hashing matches a large universe to a small space.
- Dealing with collisions
	- Check if the original strings match when there is a hit.
		- This step takes quadratic time.
		- The algorithm in the best case is linear, in the worst time quadratic. 
	- Avoid the worst-case of having many collisions with a bad **m**
		- Choose random **m**
		- m stands for modulus?
- Checking the original strings increases the complexity of analyzing the algorithm since now we need to compute the expected run time by including the cost of verification. To show that the expected run time is linear, we need to show that the probability of spurious hits is small.

## The BLAST Algorithm and Inexact Matching

In some cases it is ok to have some mismatches in a long string, but the Karp-Rabin algorithm does not allow inexact matching.

**BLAST**: Basic Local Alignment Search Tool









