# Hashing, Database Search, BLAST algorithm

> [Course video](https://www.youtube.com/watch?v=pO3GCbmfUKQ&list=PLypiXJdtIca6dEYlNoZJwBaz__CdsaoKJ&index=3)
> [Slides](https://stellar.mit.edu/S/course/6/fa20/6.047/courseMaterial/topics/topic2/lectureNotes/Lecture03_SequenceAlignment/Lecture03_SequenceAlignment.pdf)
> [0. MIT CompBio Manolis Kellis](0.%20MIT%20CompBio%20Manolis%20Kellis.md)

## Global Alignment (Review)

The Needleman-Wunsch Algorithm

**Initialization**: $F(0,0)=0$
- Top left

**Iteration**: $$F(i,j)=\max\quad\begin{cases}&F(i-1,j)-d\\&F(i,j-1)-d\\&F(i-1,j-1)+s(x_i,y_j)\end{cases}$$

**Termination**: Bottom right
## Local Alignment

The Smith-Waterman Algorithm

Problem statement: a local alignment between a string S and a string T is an alignment of a substring of S and a substring of T.

Why do local alignment:
- A gene coding for a protein may only have a portion of it that is conserved.
- Searching for a small gene in a large chromosome.
- Large segments often undergo rearrangements. Local alignment finds the regions that are conserved but in different positions.

How:
- Instead of finding a single long path, find multiple short paths that correspond to different matching pairs.
![](Pasted%20image%2020240519161544.png)

Important question: how to define an optimal local alignment? Do we include weak matches flanking the strong matches?

### Smith-Waterman Algorithm

The local alignment algorithm.

**Initialization**: $F(0,j)=F(i,0)=0$. 
- The first column and top row are all 0s
- This allows initialization from anywhere. In contrast to global alignment where $F(0,0)=0$, allowing initialization from the top left only.

**Iteration**: $$F(i,j)=\max\quad\begin{cases}&0\\&F(i-1,j)-d\\&F(i,j-1)-d\\&F(i-1,j-1)+s(x_i,y_j)\end{cases}$$Having 0 allows restarting anywhere.

**Termination**: Anywhere

### Semi-global alignment

Might have rearrangement or non-sensical sequences in one of the sequences. You want to align with at least one sequence all the way to the end.

**Initialization**: $F(0,j)=F(i,0)=0$
- Start from first row or first column

**Iteration**: $$F(i,j)=\max\quad\begin{cases}&F(i-1,j)-d\\&F(i,j-1)-d\\&F(i-1,j-1)+s(x_i,y_j)\end{cases}$$
- Same as that for global alignment. No restarting anywhere.

**Termination**: Bottom row or right column
- Must go all the way to the end for one of the sequences being aligned.

## Applying different gap penalty functions

Using a single score for all gaps is easy, but does not necessarily reflect the biological truth.

### A general gap penalty function F(gap_length)

**Initialization**: same

**Iteration**:
$$\mathrm{F(i,j)}\quad=\max\begin{cases}\mathrm{F(i-1,j-1)+s(x_i,y_j)}\\\max_{k=0\ldots i-1}\mathrm{F(k,j)-\gamma(i-k)}\\\max_{k=0\ldots j-1}\mathrm{F(i,k)-\gamma(j-k)}\end{cases}$$
**Termination**: same

The new iteration scheme allows any number of gaps. Instead of the immediate left and top cell of the current cell in the score matrix, it:
1. searches for all the cells on the top and on the left
2. add a gap penalty for each number of gaps  
3. take the max of the highest score from the top, the diagonal, and the left

This adds computational complexity, turning local computation (order of 1) to order of $n$ computations.
Do not need to be so general, counting all the top and left cells.
- Need some _smarter_ ways to estimate what gaps are possible.

### Other gap penalty functions

#### Linear gap penalty

- $w(k)=k\cdot p$
- Same penalty for all gaps
- State: current index tells if in a gap or not
- Achievable using quadratic algorithm (even with linear space)
- What we have been using previously.

#### Quadratic

- $w(k)=p+q\cdot k+rk^2$
- State: needs to encode the length of the gap, which can be O(n). Record a gap number for each position.
- To encode it we need O($\log(n)$) bits of information. Not feasible.

#### Affine gap penalty

- When a polymerase makes a mistake and create gaps, it usually skips a few nucleotides in a row.
- Trying to model this behavior, we set an initial high cost (call it $\gamma_i$) for starting a gap, then incremental cost (call it $\gamma_C$) for continuing the gap.
	- $w(k)=p+q\cdot k$, where $q<p$.
	- $p$ is the initial cost.
	- $q$ is the incremental cost.
- So for each cell in our matrix, we have two scenarios:
	- It is starting a gap
	- It is continuing a gap
- We need a _state_ for each sequence: starting a gap or not. State refers to a state in HMM, next lecture.
- We need a second matrix in addition to our original score matrix to record both scenarios
	- If we start a gap, we take $\max(\mathrm{top}-\gamma_{init},\mathrm{left}-\gamma_{init},\mathrm{diagonal}+s(x_i+y_j))$
	- If we continue a gap, we calculate the left and top scores in our second matrix using $\gamma_i$. Then take $\max(\mathrm{top}-\gamma_{continue},\mathrm{left}-\gamma_{continue},\mathrm{diagonals}+s(x_i+y_j))$

####  Length (mod 3) gap penalty

- Gaps with lengths divisible by 3 are more likely (penalized less), since they do not cause frame shifts.
- This is feasible, but requires more _states_.
- Possible states are: starting, mod 3 = 1, mod 3 = 2, mod 3 = 0
- Mod refers to modulus, or remainder?

## Linear time pattern matching

### Search for a small gene in a long chromosome with no gaps

- Doing [Local Alignment](#Local%20Alignment) of the small gene with an entire chromosome
	- This is extremely expensive! 
	- Searching for 1000 bases in a million bases require 1 billion compares between characters.
- Alternative: **Karp-Rabin** algorithm

**Linear**: proportional to the time it takes to read in the data. A string of length n takes O(n) complexity to read. An algorithm that is O(n) is proportional to the reading time. It can be 3n, 4n, 6n, etc.

#### Karp-Rabin algorithm (probabilistic linear time)

- Interpret the string numerically
- Start with 'broken' version of the algorithm
- Progressively fix it to make it work

#### Deterministic linear time solutions

- [Z Algorithm](Z%20Algorithm.md)
- Boyer-Moore and Knuth-Morris-Pratt algorithms are earliest instantiations, similar in spirit
- Suffix trees: many different variations and applications; limited use in CompBio.
- Suffix arrays: practical variation; Gene Myers, a researcher

## Karp-Rabin Algorithm

**Key idea**: Interpret string as a number. Instead of searching strings, you search for a number in a long sequence of numbers. So instead of comparing characters, you check if the numbers are equal.

Example:
T = `2359023141526739921`
P = `31415`

Moving along T, you compare if `23590 == 31415` then if `35902==31415` so on and so forth.

**Problem**: it takes linear time to compute the numbers from the numerals. To get 31415, need to do `3*10000+1*1000+4*100+1*10+5*1`.

**Solution**: compute the next number based on the previous one -> O(1)
- next_number = f(previous_number)
- easy to do
**Also Solution** : hashing (mod p) to keep the numbers small -> O(1)

### Hashing

**Problem**: The comparisons also take time. To get O(n) time, each comparison should be done in O(1) time (n comparisons between the pattern and the sequence). However, if the arguments are m-bits long ($2^m$ range, or $2^m$ possible numbers), each comparison takes O(m) time. If the argument is longer than 32 or 64 bits, they also need to be computed separately because the computer can only fit 32 bits or 62 bits at a time. The length of numbers must be reduced to something more manageable.

- Hashing maps keys $\textbf{k}$ from a large universe $\textbf{U}$ into the hash of each key $h(k)$ in a smaller space $[1\ldots m]$.
	- Here we are hashing numbers in the $2^m$ universe into a smaller space
- There are many ways possible
	- Summing all the bits
	- Take the modulus of a number
	- Take the first 20 bits or last 20 bits
- **Desired properties** of a hash function
	- Reproducibility: $x=y\Rightarrow h(x)=h(y)$ (hash of x always the same)
	- Uniform output distribution: output the hash function should uniformly span the entire smaller space
		- $x\ne y\Rightarrow P(h(x)=h(y))=\frac{1}{m}$  
		- minimizes chance of collision, which is two numbers having the same hash

**Strategy** for ensuring uniform output distribution: use all the bits. If we only use the first 2 bits, then `14123` and `14566` will have the same hash.
- mod 13: the remainder of dividing the number by 13. This operation uses all the bits.
- The modulus operation can be performed while computing the new number from the old number.
- `31415` corresponds to 7, `14152` corresponds to 8, for example.

### Collisions in hashing

- Hashing enables faster computation, but leads to spurious hits (collisions).
- When $h(x) = h(y)$ but $x \ne y$.
- Happens because hashing matches a large universe to a small space.
- Dealing with collisions
	- Check if the original strings match when there is a hit.
		- This step takes quadratic time.
		- The algorithm in the best case is linear, in the worst time quadratic. 
	- Avoid the worst-case of having many collisions with a bad **m**
		- Choose random **m**
		- m stands for modulus?
- Checking the original strings increases the complexity of analyzing the algorithm since now we need to compute the expected run time by including the cost of verification. To show that the expected run time is linear, we need to show that the probability of spurious hits is small.

## The BLAST Algorithm and Inexact Matching

### How BLAST works

In some cases it is ok to have some mismatches in a long string, but the Karp-Rabin algorithm _does not_ allow inexact matching.

**BLAST**: Basic Local Alignment Search Tool

The BLAST algorithm looks at the problem of sequence database search, wherein we have a **query**, which is a new sequence, and a **target**, which is a set of many old sequences, and we are interested in knowing which (if any) of the target sequences is the query related to.

Two key points to observe:
- Most target sequences will be completely unrelated to the query sequence, and very few sequences will match.
- Correct (near perfect) alignments will share a very long common substring.

> [!note] The Pigeonhole Principle
> The pigeon hole principle says that if you have $n$ pigeons and $n+1$ holes, there will be at least one hole with no pigeon.
> 
> This simple principle can be applied to the BLAST algorithm. If you have two sequences with >90% identity, there is only so many ways to distribute the non-matching nucleotides. Furthermore, the non-matching nucleotides will not be distributed randomly but in the non-conserved regions. Therefore, we can conclude that correct alignments will share a very long common substring.

**Steps of BLAST**:

1. Split the query into overlapping words of length $W$ ($W$-mers).
2. Find a neighborhood of similar words for each word until a threshold $T$.
	1. For example, change PQG to PEG, PRG, PKG, etc.
	2. The similarity to the original word can be defined using the BLOSUM matrix.
3. Lookup each word in the neighborhood in a **hash table** to find the location in the database where each word occurs. Call these the seeds, and let $S$ be the collection of seeds.
4. Extend the seeds in $S$ until the score of the alignment drops oﬀ below some threshold $X$.
	1. After extending the seeds, alignments are found using the [Smith-Waterman Algorithm](#Smith-Waterman%20Algorithm)
5. Report matches with overall highest scores.

> [!note] Wait! How is this fast?
> First, there are only a very small number of matches to the query sequence. So most of the searches will terminate very early.
> 
> Secondly, the hash table used in step 3 can be pre-processed offline. So it enables fast retrieval of where the seeds occur in the database.
> 
> Lastly, during seed extension, since we are considering alignment in a short region, it will not be as time-consuming as running local alignment for the entire genome.
> 

> [!note] BLAST is a heuristic algorithm
> BLAST is a heuristic algorithm, and the result might be slightly different each time. It is designed to be fast but not alway give the best alignment. 
> 
> Still pretty good in practice!

### Extensions of BLAST

Makes BLAST faster or more accurate
- **Filtering**: filtering out low complexity regions
	- low complexity regions could be found in many sequences, hence not very helpful.
- **Two-hit BLAST**: since it is more likely to have hits on two short $W$-mer rather than 1 long $W$-mer, we can use hash two short $W$-mers rather than 1 long $W$-mer. This allows us to improve speed while maintaining sensitivity.
- **Combs**: ignore some nucleotides during searching. For example, the third nucleotide in a codon often does not determine which amino acid the codon codes for. Hence, we can ignore the third nucleotide with a mask such as `110110110`. There are other combs as well.
- **PSI-BLAST**: Position Specific Iterative BLAST (PSI-BLAST) create summary proﬁles of related proteins ([PSSM](2.%20VU%20FoB%20-%20Mutation,%20Evolution,%20and%20Sequence%20Alignment.md#PSSM)) using BLAST. After a round of BLAST, it updates the score matrix from the multiple alignment, and then runs subsequent rounds of BLAST, **iteratively** updating the score matrix. It builds a Hidden Markov Model to track conservation of specific amino acids. PSI-BLAST allows detection of distantly-related proteins.

## Probabilistic interpretation of alignment scores

The **null hypothesis**: the aligned sequences are due to random chance
The **alternative hypothesis**: the aligned sequences are evolutionarily related

Accordingly:
- $Pr(\mathbf{x},\mathbf{y}\operatorname{|}U)$: P of alignment $x,y$ by model $U$ (unrelated)
- $Pr(\mathbf{x},\mathbf{y}\operatorname{|}R)$: P of alignment $x,y$ by model $R$ (related)
- $P$ that the alignment is not due to chance: $\frac{Pr(\mathbf{x},\mathbf{y}\operatorname{|}R)}{Pr(\mathbf{x},\mathbf{y}\operatorname{|}U)}$

The alignment score is the log odds ratio between probabilities of the two models: $$S\equiv\log(\frac{Pr(\mathbf{x},\mathbf{y}\operatorname{|}R)}{Pr(\mathbf{x},\mathbf{y}\operatorname{|}U)})$$
Since the score is in log scale, it means we can _add_ the alignment score of each pair of nucleotides to get the alignment score of the entire sequence, assuming each individual alignment is independent. To be more specific, see below:

For the unrelated model: $$\begin{array}{rcl}\mathbf{x}&=&\{x_1\ldots x_n\}\\\mathbf{y}&=&\{y_1\ldots x_n\}\\q_a&=&P(\text{amino acid }a)\\P(\mathbf{x},\mathbf{y}|U)&=&\prod_{i=1}^nq_{x_i}\prod_{i=1}^nq_{y_i}\end{array}$$

For the related model: $$\begin{array}{rcl}p_{ab}&=&P(\text{evolution gave rise to }a\mathrm{~in~}\mathbf{x}\mathrm{~and~}b\mathrm{~in~}\mathbf{y})\\P(\mathbf{x},\mathbf{y}|R)&=&\prod_{i=1}^np_{x_iy_i}\end{array}$$
Therefore: $$\begin{array}{rcl}\frac{P(\mathbf{x},\mathbf{y}|R)}{P(\mathbf{x},\mathbf{y}|U)}&=&\frac{\prod_{i=1}^np_{x_iy_i}}{\prod_{i=1}^nq_{x_i}\prod_{i=1}^nq_{y_i}}\\&=&\frac{\prod_{i=1}^np_{x_iy_i}}{\prod_{i=1}^nq_{x_i}q_{y_i}}\end{array}$$

If we take the log of products, we get summations: $$\begin{aligned}&S&&\equiv\quad\log\frac{P(\mathbf{x},\mathbf{y}|R)}{P(\mathbf{x},\mathbf{y}|U)}\\&v&&=\quad\sum_i\log\left(\frac{p_{x_iy_i}}{q_{x_i}q_{y_i}}\right)\\&&&\equiv\quad\sum_is(x_i,y_i)\end{aligned}$$

Therefore, the score for a pair of alignment between $a$ and $b$ is: $$s(a,b)\quad=\quad\log\left(\frac{p_{ab}}{q_aq_b}\right)$$

It is interesting to note that the score of a match of an amino acid with itself depends on the amino acid itself because the frequency of random occurrence of an amino acid aﬀects the terms used in calculating the likelihood ratio score of alignment. Hence, these matrices capture not only the sequence similarity of the alignments, but also the chemical similarity of various amino acids.

## Deterministic linear time exact matching

The [Karp-Rabin Algorithm](#Karp-Rabin%20Algorithm) is probabilistic string matching because hashing collisions introduce a false positive rate. There are also algorithms for deterministic exact matching.

TBD later.














