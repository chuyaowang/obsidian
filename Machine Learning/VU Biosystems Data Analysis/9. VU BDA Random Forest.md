# Random Forest

[0. VU BDA](0.%20VU%20BDA.md)

## Aims

- Decision tree algorithm
- Bagging algorithm
- RF algorithm
- How aggregated decision trees work in the context of regression and classification tasks
- Assess the validity of an aggregated decision tree model
- Relevant parameters that can be set for an aggregated decision tree model

## ML in a nutshell

- Predict response variables Y from X, a samples by features matrix
- A model is a _function_ that connects X to Y
- Regression: Y is continuous values
- Classification: Y is class labels
- Test-train split
- Loss function: describes the performance of model by comparing the predicted response to the real response

## Decision trees

### Regression

#### Example

- Predict age from CpG sites
- X: sample by CpG site matrix, records methylation percentage
- Y: age
- First check if the dataset is balanced in gender, the range of age
	- learns from both genders
	- covers a long age range
- Question: what age would the simplest model predict (`lm(Y~1)`)
	- mean age

#### Decision tree

- Divide the samples into subsets using thresholds of variables
	- For the example, find thresholds of CpG methylation percentage to divide the samples into young subset and old subset
- Prediction is the _mean value_ of the response variable in each subset
- The threshold is found by minimizing the total residual sum of squares of the subsets: $$RSS=\sum_{j=1}^{J}\sum_{i\in R_{j}}(y_{i}-\hat{y}_{i})^{2}$$
	- For continuous variables, rank the values of the variable (CpG site)
	- Test all possible splits, i.e. between two values 1 and 2, test 1.5
	- Calculate the total RSS
		- $i$: each sample
		- $y_i$: response of the sample
		- $\hat{y}_i$: mean response of the subset, also the predicted value
		- $j$: each subset
		- $J$: total number of subsets, 2 for a binary split
	- For discrete variables, test all ways to split the classes
		- For example, classes A, B, C can be split into A and BC, AB and C, C and BA
- Within each subset, keep dividing using the variables. It can be variables already used for making splits or other variables.
- Keep dividing until a stop criterion is met:
	- the max tree depth or number of nodes is reached
	- the number of samples in a node falls below a certain threshold
	- the variance reduction from further splits is below a certain threshold
	- all samples in a node have the same response value

> [!note]- Variance reduction criterion
> Another way to choose the threshold is using variance reduction:
>    $$\text{Variance Reduction} = \text{Var}(y) - \left( \frac{n_{\text{left}}}{n} \cdot \text{Var}(y_{\text{left}}) + \frac{n_{\text{right}}}{n} \cdot \text{Var}(y_{\text{right}}) \right)$$
>    - $\text{Var}(y)$: Variance of the target variable in the current node.
>    - $\text{Var}(y_{\text{left}})$ and $\text{Var}(y_{\text{right}})$: Variances of the target variable in the left and right subsets after the split.
>    - $n$: Total number of samples in the current node.
>    - $n_{\text{left}}$ and $n_{\text{right}}$: Number of samples in the left and right subsets.

#### Prediction

- To make a prediction for a new input, traverse the tree from the root to a leaf node based on the feature values of the input.
- The predicted value is the mean of the target variable in that leaf node.

#### Evaluate model performance

- During **training**: more optimistic because the model is trained on the data
- During **testing**: closer to real use case
- Evaluate using root mean square error (RMSE): $$RMSE=\sqrt{\frac{\sum_{i=1}^N(\hat{y_i}-y_i)^2}{N}}$$
	- $\hat{y}_i$: predicted value for the sample
	- $y_i$: true value of the sample
	- $N$: total number of samples

#### Hyperparameters

- Number of leaf nodes
- Tree depth
- Minimum decrease in RMSE of a split
	- is the RMSE reduction caused by the split greater than a threshold
- The more leaves there are, the deeper the tree depth, and the lower the minimum decrease is, the **less** generalizable the model is.

#### Is decision tree the proper model

- Observe how the response correlates with the variables
- If it correlates well, probably better to use linear regression

### Classification

#### Example

- Classification of samples into treatment conditions based on gene expression values
- What would the simplest model predict?
	- When all classes have the same number of samples, the simplest model predicts a randomly selected condition from all classes

#### Decision tree

- Find thresholds to split the data into subsets
- The prediction is the majority class in the subset
- Threshold is chosen by maximizing classification accuracy within each subset
	- Recursive binary splitting
	- Accuracy of a two-class problem: $$\frac{TP+TN}{N}$$
		- [Confusion Matrix](Confusion%20Matrix.md)
	- Multi-class accuracy: $$Accuracy_{multi}=\sum_{i\in C}^C\frac{N_{correct}^i}{N_{samples}}$$
		- calculate the percentage of correct predictions for each class
		- sum over all classes
		- $N_{correct}$: correct predictions in class $i$
		- $N_{samples}$: total number of samples
		- $C$: total number of classes
- Problem with decision tree algorithm: it cannot think ahead
	- If a slightly worse split at the current step enables a much better split in later steps, the algorithm cannot find that
- Keep dividing until a stopping criterion is met

#### Hyperparameters

- Same as [Regression](#Regression) trees

#### Evaluate model performance

- Also with multi-class 

### Pros and cons

- Pros
	- Simple to understand and interpret
	- Can handle both numerical and class data
	- Require little data preparation
- Cons
	- Trees can be unstable: struggle when points mix
	- Greedy: cannot look ahead
	- Prone to overfitting: need to regularize by limiting tree depth or number of nodes

## Bagging

### Model aggregation

- Model the data in various ways simultaneously
	- Multiple decision trees, decision trees + neural network, etc.
- Each model can focus on a different aspect of data
- Combine different models using aggregation
- The principle can be applied to more than just decision trees

### Bagging of decision trees

- Split data into parts
	- Sample with replacement
- Give each mini-tree a part
	- Every tree sees a part of samples
	- Every tree sees all variables
- Majority voting for classification

### Bagging improves model performance

- Makes the prediction more reliable
	- **Reduces variance**: Individual decision trees have high [Variance](Machine%20Learning/Concepts/Bias%20Variance%20Trade-off.md#Variance). Bagging reduces this variance by averaging the predictions of multiple trees. Errors in individual trees tend to _cancel each other out_ after averaging. This makes the model generalize better to other datasets.
		- For example, an overfitting tree cancels out with an underfitting tree
	- **Introduces diversity**: By training trees on different subsets of the data, bagging ensures that the trees are not all making the same mistakes. This diversity helps the model capture a _broader range of patterns_ in the data.
- Reduces greediness
	- Since the samples are split into parts, the greediness is contained to a smaller range. Each tree learns a different split. Combined together, the overall greediness is reduced.
- Reduces overfitting
	- While a single tree might overfit to specific noise or outliers in the training data, the ensemble of trees is less likely to overfit because the overfitting tendencies of individual trees are averaged out.

### Out of bag samples

- Since each tree only sees its own portion of data, the other portions (called out-of-bag samples), can be used to validate the tree.

### Hyperparameters

- Number of trees
- Tree depth
- Number of leaf nodes
- Minimum improvement in a split

### Evaluation

- Evaluate model performance: RMSE or multi-class accuracy
	- Using training set data
	- Using out-of-bag data
	- Using test set data
- Feature importance: how important is a feature in making the prediction

#### Bagged classification

- Accuracy of each class vs. number of trees plot
- Accuracy goes up and down observed
	- Because sometimes an added tree does not predict a class well, and the bad result affects majority voting
	- The added tree may not have the data for the correct class or only a small number of it to learn the pattern
- Overall, more trees lead to higher accuracy, but there is a upper limit

#### Feature importance

- Mean decrease in accuracy: decrease in multi-class accuracy after removing the feature, calculated for each tree then averaged

### Bagged regression

- Take the mean of predicted response value of all trees
- Predicts more continuous response values compared to the discrete values for a single decision tree

### Overfit of bagging

- the trees are very deep 
	- Very deep trees can still overfit on its subset
	- each tree can still learn noise from data
- the number of trees is very large
	- Having too many trees and a small data set means some trees may see the same or very similar subset. Averaging them will not reduce the model variance much.
	- when the trees are deep, having many trees will reinforce the noise pattern learned
- correlated trees
	- dominant features make the trees behave very similarly. They act like a single deep decision tree

## Random forest

- Subset the data by sample and by feature

### Hyperparameters

- Number of trees
- Number of variables for each tree
- Number of samples for each tree
- Max number of leaves per tree
- more

### Evaluation

- Hard to evaluate and interpret individual trees
- Evaluate using aggregated metrics: RMSE, multi-class accuracy of the whole model

### Pros and cons

#### Pros

- Can handle both numerical and class data
- Require little data preparation
- Better protected against overfitting than decision trees
- Better performance than decision trees

#### Cons

- Not easily interpretable
	- Cannot see individual trees
	- Evaluate only with aggregated statistics

