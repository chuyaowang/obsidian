# Validation

[0. VU BDA](0.%20VU%20BDA.md)

## T test

Is my feature able to discriminate two groups.

T-distribution: all possible t values when the null hypothesis is true.

Assumes the data are normally distributed.

### QQ-plot

- Checks for normality of data.
	- Normality often required for parametric test, like t test
- Plots theoretical quantiles vs. data quantiles
	- Given n samples, calculate z values for $p = [\frac{1}{n+1},...,\frac{n}{n+1}]$
	- Calculate z scores from data
- Make plot
- Look for deviation from diagonal

## Wilcoxon Mann Whitney Test

Non-parametric test
Uses rank of data instead of actual values

## Permutation testing ?

- Permute the data many times
- Non parametric
	- No equal variance
	- Any difference measure can be used, such as squared distance between group average

## Multivariate methods

- Use a combination of features to discriminate between groups

### PLSDA

- easy to overfit the data
- steps:
	- weights are calculated by [Inner product](Inner%20and%20Outer%20Product.md#Inner%20product) of class information with each variable; $w=X^Ty$
	- scores (like the ones in LDA) are calculated by taking a weighted sum of variables; $T_{PLS}=Xw$
	- LDA on the scores

### PCDA vs. PLSDA


##  How many components for PCDA/PLSDA

- too many: introduce noise
- too few: not enough information
- choose number of components that lead to the lowest prediction error for new samples
	- cross-validation

## Cross-validation

- Remove a subset of data
- Make the model
- Test on the left-out set
- Choose _different number of latent variables_ and evaluate errors
	- RMSEC: RMSE for calibration
	- RMSECV: RMSE for cross validation
	- RMSEP: RMSE for test set prediction

### 1 CV or 2 CV cross validation

- 1 CV
	- separation into _train and validation_ sets
	- make multiple models using different number of components
	- validation set predictions are used to choose the optimal number of components
	- report the validation error
	- some data leakage; underestimates prediction error
- 2 CV
	- separation into _train, validation, and test_ sets
	- choose optimal number of PCs using validation set predictions
	- build new model on train + validation samples with optimal number of PCs
	- evaluate on test set and report error; realistic for new samples

### Leave one out CV

- Often _too optimistic_. When samples > 20 leave more out.

### Leave more samples out

- Default is to make 7 groups of samples and leave 1 group out
- Can be continuous, venetian blind, or random
	- Sometimes continuous samples carry information on samples; time series, etc.

## How many components for PCA

### Normal cross validation gives too optimistic results

- 

### Cross validation PCA

- 

## Is my feature able to discriminate two groups with multivariate data

- is it different from chance data
- perform classification on chance data
- compare number of misclassifications to the number of misclassifications in real data 

### example


### MUVR

- optimal models using cross model validation and feature selection
- remove unimportant variables as long as model improves
- max model: using all relevant features
- mid model
- min model: using minimal optimal features only
- which to choose depends on the goal: pathway analysis, etc.

### H0 distribution of regression coefficients

- collect H0 regression coefficients from permutation models
- compare original regression coefficients with H0 distribution
- select variables that are outside confidence interval

## Ensemble predictions

- In cross model validation many models are developed
- Each model results in a regression coefficient that can be used to predict class
- majority voting