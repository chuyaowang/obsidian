# Neural Nets

[0. VU BDA](0.%20VU%20BDA.md)

## Examples

- AlphaFold: predict protein folding
- Enzyme function prediction
- Single cell omics analysis
- Antibiotics activity prediction

## Linear model for regression

$$\hat{y}=w^Tx+b$$
- $w$: weights
- $b$: bias
- A linear layer
- More complex: more layers, more nodes

### Loss function

- Squared error loss on a single point: $\frac{1}{2}(\hat{y}^i-y^i)$
- For all datapoints: $\frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}(\hat{y}^i-y^i)$
- Goal: find weights that minimize total training loss

## Gradient descent

### Minimization of loss

- Use derivative to update weights in the direction that lowers the loss function
- Make use of the derivatives of the loss function, $\frac{\partial L}{\partial w}$

### Partial derivative and gradient

- Partial derivative: $\frac{\partial L}{\partial w}$, derivative of loss with respect to a weight
- Gradient: $$\nabla_\mathbf{w}f(\mathbf{w})=\left[\frac{\partial f(\mathbf{w})}{\partial w_1},\frac{\partial f(\mathbf{w})}{\partial w_2},\ldots,\frac{\partial f(\mathbf{w})}{\partial w_d}\right]$$
- Negative gradient points towards minimum

### Autograd

- Symbolically represent local derivatives
- Compute numerical values of local derivatives
- Build computational graph to track flow of tensors through operations (matrix multiplication, activations, etc.)
- Chain local derivatives through the computational graph; gets the derivative of loss with respect to each input and weight

### Stochastic gradient descent

- Initialize the values of the model parameters; can be done at random
- iteratively sample random mini-batches from the data
- update parameters in the direction of the negative gradient
	- $$(\mathbf{w},b)\leftarrow(\mathbf{w},b)-\frac{\eta}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\partial_{(\mathbf{w},b)}l^{(i)}(\mathbf{w},b)$$
	- take the mean gradient over the mini-batch
	- $\partial_{(\mathbf{w},b)}l^{(i)}(\mathbf{w},b)$: partial derivative of the loss wrt. $w$ and $b$
	- $\eta$: learning rate

### Parameters

- Internal: variables of the model (learned during training)
	- in neural network: weights and biases
- Hyperparameters
	- external configurations of training; fine-tuned
	- learning rate
	- batch size
	- epoch number

### Sources of stochasticity in SGD

- Initial values of parameters
- How samples are batched

What does this mean for multiple minima?
- May be stuck in local minimum
- Random restarts may not end in the same minimum
- Learning rate too high: miss a minimum
- Learning rate too low: can't climb out of a minimum

## Softmax regression for classification

### One-hot encoding

- represent categorical data
- ex. 3 classes are represented as $y\in\{(1,0,0),(0,1,0),(0,0,1)\}$

### Softmax regression

- [Softmax](Softmax.md) maps to values between 0 and 1
- Put output through softmax to convert to probability of being each class

### Cross entropy loss

[Cross-Entropy Loss](Cross-Entropy%20Loss.md)

## Training

- Training set: trains the model
- Validation: 
	- evaluates the model after each epoch or batch; 
	- early stopping;
	- tuning hyperparameter tuning;
	- no training is done in the process.
- Test: used once after training and hyper-parameter tuning are finalized

## Hidden layers

- Multiple hidden layers for more non-linearity
- Activations: sigmoid, ReLU, Tanh, linear