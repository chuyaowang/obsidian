# Linear Algebra for Data Analysis

[0. VU BDA](0.%20VU%20BDA.md)

## Important topics

[1. The Column Space of A Contains All Vectors Ax](1.%20The%20Column%20Space%20of%20A%20Contains%20All%20Vectors%20Ax.md)
[2. Multiplying and Factoring Matrices](2.%20Multiplying%20and%20Factoring%20Matrices.md)
- Ax=b
- Ax=0
- Null space, column space, row space
- Rank
- Inverse

This lecture: understand these concepts related to data matrix.

## Terms

- X: data, $I\times J$
	- $X_i$: $i$-th row of X
	- $X_j$: $j$-th column of X
	- $X_{ij}$: a single measurement
- y: response, $I\times 1$ or $I\times k$

## Regression/Linear Model

- $J$ metabolites measured from subjects $X$ to predict complex phenotypic property $y$. Prediction error (residual) $e$
- $$y=Xb+e$$

### Vectors y, e, b as directions in space

- vectors as directions in space
- the numbers define the direction and length of vectors
- vectors will always be defined as a column

### Vector: point in space

- a scalar represents a point on a 1 dimensional space in a known direction with a given unit
- a vector represents a direction point in a 2 dimensional space
	- each dimension can have its own unit
- higher dimensional vectors are points in higher dimensional space

### Adding and subtracting vectors

- visualization in space
- adding and subtracting can only be done when the vectors have the same size
- **Mean centering**
	- subtract the mean of vector from each element in the vector
	- the variation between values is often represented by their mean-centered value

### Multiplication by a constant

- multiplying a vector by a constant

### Data matrix X

- Rows: samples
- Columns: features
- A 3 by 2 matrix can be represented as:
	- 3 points on a 2 dimensional space: in the row space; sample space
	- 2 points on a 3 dimensional space: in the column space; feature space
- The row space has dimension $J$: each row has $J$ elements
- The column space has dimension $I$: each column has $I$ elements

### Inner product

[Inner product](Inner%20and%20Outer%20Product.md#Inner%20product)

- $a\cdot b$: length of $a$ times the orthogonal projection of $b$ on $a$
- If dot product is 0, $a$ and $b$ are orthogonal
- the orthogonal vectors of $a$ makes up an orthogonal space

### Distance

- Distances between 2 points: subtract $b$ from $a$
- [Euclidean Norm](Euclidean%20Norm.md)
- Angle between 2 points: [cosine similarity](Pearson%20Correlation.md#^2d5a55)

### Dot product applied in regression

- $\hat{y_i}=X_i^Tb$
- This can be viewed as a dot product
- The length of $b$ times the length of $X_i$ **projected** on $b$.
- $\hat{y}$ gets larger when $X_i$ is longer and when the angle between $X_i$ and $b$ is smaller
- $\hat{y}$ is 0 when $X_i$ and $b$ are orthogonal
  
### Rank of a matrix

- **Property**: multiplication of a matrix cannot increase the rank
	- If two matrices are multiplied, the rank of the product is at maximum the lowest rank of the two matrices.
- (Chemical) rank: with noisy data the rank is often considered to be the dimension of the systematic variation (disregarding the noise)
- The rank is the number of vector pairs that reproduce the matrix. If a matrix is obtained by $uv^T$, $m\times1$ by $1\times n$, since matrix multiplication does not increase its rank,  the matrix has rank 1. It takes 1 pair of vectors to reproduce this matrix.
- The maximum rank of a matrix is the minimum of its size. Thus, a $3\times 2$ matrix $\mathbf{X}$ has maximum rank 2 (but could also be rank 1). 
### Geometry: rank/subspace

$$\begin{bmatrix}3&2&1\\1&1&1\end{bmatrix}$$
- fill in later

### Outer product


## Matrix multiplication


### Inverse of a matrix

- [Inverse matrices](3.%20Multiplication%20and%20Inverse%20Matrices.md#Inverse%20matrices)
- Inverse matrix exists only for full rank matrix

### Correlation matrix

- $X^TX=D$
-  

## Singular value decomposition

