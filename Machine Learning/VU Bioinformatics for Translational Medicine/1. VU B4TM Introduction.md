# Introduction

[0. VU B4TM](Machine%20Learning/VU%20Bioinformatics%20for%20Translational%20Medicine/0.%20VU%20B4TM.md)
> Evert Bosdriesz

## Logistics

- Lots of guest lectures
- Practicals: 
	- CATS (40%)
	- differential gene expression analysis (5%)
- Exam (55%)
	- Questions about guest lectures
	- Workflow from 2 guest lectures of your choice

## Translational medicine

- Translating fundamental biology to clinic
	- finding biomarkers
	- framing biological hypothesis
	- finding drug targets
	- clinical trials
- Biological data: Store, process, analyze, model, predict
- Personalized medicine: tailor therapy to individuals or groups of individuals
	- Example: breast cancer subtypes
	- [The State of Precision Medicine 2025](Biology/Seminars/The%20State%20of%20Precision%20Medicine%202025.md)

### Data

- Genomics (DNA):
	- point mutations, small variants (exome sequencing, DNAseq)
	- structural variants (arrayCGH, DNAseq)
	- epigenetics: methylation
- Transcriptomics (mRNA):
	- (single cell) RNA seq
	- micro arrays (becoming obsolete)
- Proteomics (proteins):
	- immunohistochemistry: antibody based
	- mass spectrometry (MS/MS)
- Metabolomics (metabolites, small molecules):
	- NMR, mass spectrometry
- And much more

## Biomarkers

- Omic data is often used to find biomarkers
	- A _metabolite_ or _protein_ present in blood/urine/stool
	- _Mutation_ found in blood or a tumor sample
	- Over or under expression of a set of _genes_
- Application of biomarkers
	- **Diagnosis**: what is the disease; cancer detection, disease (sub)classification
	- **Prognosis**: what is the outcome; chance of survival
	- **Treatment plan**: which drug should the patient get; chemotherapy or not

### Kaplan Meier curve

- A KM curve shows the _fraction of surviving patients_ over time
- Typically used for progressive diseases
- A useful _prognostic_ biomarker will separate the survival curves

## CATS assignment

- Classification assessment of tumor subtypes
- Give predictions and estimate how many predictions are correct

> Task: build a classifier to separate to subtype breast cancer samples based on copy number data derived from arrayCGH

### Example: breast tumor classification

- Dutch cancer institute (NKI)
- Make prediction on high or low risk of metastasis. Metastasis means bad prognosis
- Breast cancer treated with chemotherapy. Need to identify patients that need or do not need chemotherapy.
- RNA microarray experiment: 117 patients x 25000 genes; labeled with good vs. bad prognosis

### Deliver

- Classifier (15%, predictions and accuracy estimate)
- Write a paper (70%)
- Presentation of results (15%)
- Peer review

## Cancer subtypes

- Hormone receptor
- HER2 receptor
- 3 subtypes
	- **Triple negative**: HR-/HER2-, 13%
	- **Luminal B**: HR+/HER2+, 10%
	- **HER2-enriched**: HR-/HER2+, 5%
	- **Luminal A**: HR+/HER2-, 73%
	- thought: as multi-label classification problem?

## Copy number data from arrayCGH

- Tumors are often chromosomally unstable
	- They can have more or less DNA $\Rightarrow$ **copy number variation (CNV)**
	- Losses and gains of entire chromosomes or structural variations

### arrayCGH

- Comparative genome hybridization (CGH)

![[Media/1. VU B4TM Introduction 2025-04-08 12.09.56.excalidraw]]

### Pre-processing

- Instead of looking at each nucleotide, the data can be _segmented_ into ranges of high and low log ratios
- After segmentation, define each segment to be _gain_, _loss_, or _normal_
- Dimension reduction: combine segments with similar CNV profiles

### Summary

- Tumors have chromosomal aberrations (copy number variation)
- The copy number of each piece of DNA can be quantified
- The data is segmented in regions of gains (+1), amplifications (+2), normal (0), and losses (-1) of DNA with respect to normal DNA

## Classifier

- ML: unsupervised and supervised
- Unsupervised: no label
	- Clustering
	- PCA
	- TSNE
	- Auto-encoders
- Supervised: has label
	- Classification
	- Regression

### Basic principles of classification

- Class label $Y$
- Feature vector of predictor variables $X$
- Predict $Y$ from $X$
- Ex:
	- $Y$: $\{\text{red},\text{square}\}$, classes
	- $X$: $\{\text{color},\text{shape}\}$, features

### Training

- Train model using training set and make predictions on new data

### Assignment recap

- Data: copy number from arrayCGH
- Labels:
	- **HER2 positive**: HER2+
	- **Triple negative**: ER-, PR-, and HER2-
	- **HR+**: ER+ and/or PR+, and HER2-
- Predict the label based on the arrayCGH profile of the samples alone

### Classification techniques

- Naive bayes
- Random forest
- Linear discrimination
- SVM
- Nearest mean
- Many more

### Nearest mean classifier

- Calculate the mean of each group
- Calculate the distance between the new sample and the means
- The new sample is classified to the group that it is closer to
- Issues:
	- New point can be right on the decision boundary. Then its label cannot be determined
	- Many more features than samples. Not enough samples to learn from features. This can lead to overfitting.
		- Classic rule of thumb: number of samples per variable should be > 10. If we have 35 samples, at most 3.5 variables should be measured.

### Two approaches for building a classifier

- Feature selection or regularization: avoid overfitting on noise
- Cross validation: how does it perform on new data

### Cross validation

- 5 fold cross validation: train on 4 fold and validate it on the other 1 fold
- leave 1 out cross validation: train on all but 1 sample, and predict on that 1 sample
- Quantify:
	- training error: error during training
	- cross validation error: error during validation
	- test error: error on test data
- Example of a more extensive cross validation scheme: [here](https://pubmed.ncbi.nlm.nih.gov/15817694/)
