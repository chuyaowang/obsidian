# Assignment notes

## Pre-processing

- binary biomarkers were transformed into a categorial-biomarkers score

## Feature selection

- Knowledge based: translate chromosome region to genes -> KEGG pathway labeling or GO ontology -> filter for cancer related pathways
- Univariate way: 
	- logistic regression -> rank weights, filter for significance after adjusting for multiple testing (ex. Benjamini Hochberg)
	- Wilcoxon rank sum test -> rank p values
	- Variance inflation factors and threshold
- Multivariate way: PLSDA -> rank VIP value
- XGBoost can select variables as a part of regularization

## Evaluation

- Sensitivity, specificity, accuracy
- AUC
- Loss function

## Interpretation

- XGBoost: SHAPley values
	- trend of SHAPley value as a function of feature value
- pathway enrichment

## Miscellaneous

- Check for case control balance: checked, balanced
- Net reclassification improvement (NRI): an index that compares different classification approaches

## Writing

I am writing a paper-like project report and I need you to write my scheme of feature selection, tuning of XGBoost parameters, validation of the optimal parameters, and training the final model into the methods section of the paper. I have attached my R markdown files. Read the code and understand it. The tune_xgboost.Rmd contains feature selection and tuning of xgboost parameters. The train_xgboost.Rmd contains validation of optimal parameters and training the final model. The writing should explain the models used (splsda and xgboost), why splsda feature selection is necessary even if xgboost has regularization capabilities, how the models were used (but without going into the details of the actual functions themselves), the important parameters of splsda and xgboost, the cross validation to tune the splsda parameters, the cross validation to tune xgboost parameters, and the cross validation to evaluate the final set of parameters. The language should be succinct, professional, suitable for academic publication, and does not sound AI generated. The writing should have a coherent logical flow and try to fit into the following 3 subtitles: Multivariate feature selection using SPLSDA, Fine-tuning SPLSDA parameters, Fine-tuning XGBoost parameters, Evaluation of Final Parameters, and Training XGBoost Model. Keep the number of words below 1000.

### Methods

#### Multivariate Feature Selection Using SPLSDA  
Sparse Partial Least Squares Discriminant Analysis (SPLSDA) was employed for feature selection to enhance model interpretability and mitigate high-dimensional noise. While XGBoost inherently performs feature regularization, SPLSDA was integrated to explicitly identify stable biomarkers and reduce redundancy, ensuring biological relevance. SPLSDA operates by projecting features into latent components while enforcing sparsity, retaining features with the highest discriminative power. Key parameters included the number of components (*ncomp*) and features per component (*keepX*), which were optimized to balance model complexity and performance. Features were further filtered based on stability across cross-validation folds, retaining those selected in >25% of iterations to ensure robustness.

#### Fine-Tuning SPLSDA Parameters  
The optimal *ncomp* (number of components) and *keepX* (features per component) were determined via repeated cross-validation (10 folds, 10 repeats). Performance was evaluated using the overall balanced error rate. Candidate values for *keepX* spanned 1–200, and *ncomp* was capped at 3 to avoid overfitting. The final parameters minimized classification error while prioritizing feature stability, ensuring generalizability to unseen data.

#### Fine-Tuning XGBoost Parameters  
XGBoost hyperparameters were optimized through a grid search evaluating learning rate (*eta*: 0.1, 0.3), maximum tree depth (8–10), subsample (0.9–1.0), and column subsampling (0.9–1.0). A 5×10 nested cross-validation framework was implemented: SPLSDA-filtered features were recomputed for each fold to simulate independent training conditions. Model performance was assessed using the mean classification error rate (*merror*), with early stopping after 10 rounds of non-improvement. This approach ensured parameter robustness against feature variability and data partitioning.

#### Evaluation of Final Parameters  
The optimal SPLSDA and XGBoost parameters were validated using an independent 5×10 cross-validation. For each fold, features were reselected using SPLSDA, and the XGBoost model was retrained. Performance metrics included overall error rate, per-class balanced accuracy, and one-vs-all AUC. Results were aggregated across folds to compute average confusion matrices, error distributions, and ROC curves, ensuring unbiased estimates of generalization performance.

#### Training XGBoost Model  
The final model was trained on the full dataset using the validated parameters. SPLSDA-derived features were selected with stability thresholds (10×10 cross-validation) to construct a reduced feature matrix. XGBoost was configured for multi-class classification (*softprob* objective) with a maximum of 200 boosting rounds. The model was saved alongside feature selection filters to ensure reproducibility. This two-stage workflow—feature selection followed by ensemble learning—enhanced interpretability while maintaining high predictive accuracy.  

## Results

### Increased SPLSDA performance after hyperparameter tuning

I have an almost finished methods section and written up a draft for the results section of the paper, both in the attached file. The results draft discusses results and figures from SPLSDA hyperparameter finetuning, feature selection using SPLSDA, XGBoost hyperparameter finetuning, XGBoost performance evaluation, and XGBoost feature importance analysis using SHAP analysis. Read the draft through and interpret its meanings. I need you to revise the draft to 1. Fill in the section classification using XGBoost in the updated methods section in language and tone similar to other sections of the methods section. Leave other sections of the methods section untouched. 2. generate informative figure titles and captions as well as table titles and/or captions. 3. break down the results and its discussion of the figures into 3 major sections: SPLSDA, XGBoost, and SHAP analysis and fewer than 5 subsections within each major section. Expand and connect the bullet points into paragraphs. Do not remove important insights and discussion of the figures. 4. Choose section titles and subtitles that are short, informative, and is a one-line summary of the result. 5. Reorder the information if needed to keep a logical and coherent storytelling and avoid redundancy (mentioning the same thing repeatedly). 6. Follow the guidance in the {} brackets that I left in places of the draft but do not include them in the main text 7. Make sure the discussion made here resonates with the logical flow of the methods section 8. Revise the language such that the overall tone should be succinct, professional, suitable for academic publication, and does not sound AI generated. 9. Keep words around 1500 to 2000. 10. Output 1 section first, after I say proceed, proceed to the next section



- The SPLSDA feature selection process is demonstrated first
- Figure 1: a figure showing 6 plots. The top 3 plots splsda result of component 1 and 2 for splsda using preliminary parameters (ncomp=10, all features are used) (Figure 1A), using optimized parameters (Figure 1B), and using filtered features. The circles surrounding the points are 95% confidence ellipses. Figure 1 D-E are corresponding 3D plots for figure A-C to aid visualization.
- The preliminary SPLSDA projects samples from the 3 groups HER2+, HR+, and Triple Neg in 3 clusters with overlap. Furthermore, the variance explained by the first 3 components is only about 23.5%. Much of the variance in the data are from noise and does not explain the separation between the groups. Hence, the model struggles at learning the true separation pattern between the groups. After hyperparameter tuning, the HER2+ group is visibly more separated from HR+ and Triple Neg. This is the result of hyperparameter tuning, which reduces the number of components and the number of features used to increase classification accuracy. However, the percent variance explained decreases to about 16%, suggesting that some features may still be noise and not actually biologically relevant ones that are different between the cancer subtypes. {there is still figure 1C which is splsda result after feature selection to explain and interpret here, but I cannot do it without first go through how hyperparameters are tuned and how features are selected. Try to resolve this incoherence in storytelling in your revision}
- Figure 2: a figure displaying hyperparameter tuning results. 
- The tuning uses an iterative algorithm that tests the optimal _keepX_ (from the range 1-200) for each number of _ncomp_ (1, 1 and 2, 1 and 2 and 3) using 10 fold cross validation for 10 repeats. Iterative means that the optimal keepX for component 1 is used when testing the optimal component for component 1+2, and so on. The classification error rate is the y axis of the figure. The number of selected features is the x axis. 3 curves correspond to 3 different number of components. The error bars on the curves represent the confidence interval of the corresponding classification error rate for the combination of ncomp and keepX, obtained through the 10 fold cross validations for 10 repeats. The combination of ncomp and keepX that achieved the lowest classification error rate is chosen (ncomp: 3, keepX: 190, 5, 190, note that a feature can be used for multiple components so there is overlap in the feature number defined by keepX. The true number of unique features used is probably less than 190+190+5 = 385).
- From figure 2 we can see that while for component 1 the optimal number of features is 190 (labeled by the larger diamond shape on the curves) with a high degree of certainty demonstrated by the continuously decreasing classification error rate towards 190 features, for component 2 and 3 the slope around the optimal point is quite flat, and the optimal number of features can vary a bit depending on how the data were partitioned during cross validation {in fact the figure being displayed is a demo one. I did not save the figure corresponding to the hyperparameter tuning that lead to ncomp: 3, keepX: 190, 5, 190. The optimal features in the demo figure is 80. Try to mention this in the figure caption or the text subtly, like in a parentheses}. This means that when we run cross validation later to optimize XGBoost hyperparameters while keeping the splsda hyperparameters fixed, this variability is not accounted for in the cross validation. This could lead to an underestimate of model variance from the cross validation and an overestimate of model performance. However, the cross validation for XGBoost hyperparameter tuning still account for variance from splsda feature selection, variance from different combination of XGBoost learning rate, tree depth, row subsampling, and column subsampling, and variance from different folds of data. So the overestimate should not be too great. Furthermore, the splsda hyperparameters are chosen from cross validations as well. So it should also be robust for unseen data, not leading to too much performance drop in the test data.
- Figure 2 also shows that for component 1 and 3 a high number of features are needed to achieve low classification error rate, while for component 2 only a small number is needed. This can be understood by looking at figure 1A and 1B. After hyperparameter tuning, the groups HR+  and Triple Neg are still hard to separate on component 1 and 3, thus requiring many features, while on component 2, HER2+ becomes visibly more separated from the other 2 groups with only 5 features contributing to component 2. The large classification error rate drop with only 5 features suggests the difference between HER2+ vs. the other 2 groups is determined by only a few factors. 
- Figure 3: the correlation circle plotting the loadings vector of component 1 and 2. The outer circle has a radius of 1 and the inner circle has a radius of 0.5.
- Still, since the total explained variance of the components are still low, we can expect some features are still just noise. This conclusion is confirmed in figure 3. While only one feature (17_35076296_35282086) dominates component 2, component 1 has many features contributing to it, most of which are within the radius 0.5, having only small contributions. We can further filter them using loadings and by performing 10 x 10 cross validation and calculate how many times a feature selected in all the cross validations, which is termed feature stability.
- Figure 4: a composite figure with A-C displaying feature stability distributions in barplots for component 1, 2, and 3. Figure 4 D-F displays loadings distribution for componet 1, 2, and 3 using horizontal barplots. The loadings bars are colored as the group in which the feature has the highest mean value. 
- Figure A-C show that only a small fraction of features are selected almost in all cross validations. Similarly, most of the loadings are small comparing to the loadings of the top contributing features. {In figure 4D about half the features have highest mean value in HR+ and half in Triple Neg. In figure 4E the top contributing feature 17_35076296_35282086 is the highest in HER2+ and other 4 features have neglible contributions have the highest mean value in Triple Neg. In figure 4F most of the features have the highest mean value in Triple neg with only a few in HR+ and HER2+. Interpret what this means for the 3 components and add to the discussion}. To avoid hurting downstream model performance, we use a lenient frequency threshold of 0.25 and a dynamic loadings threshold using the median loading for a component as the threshold for filtering. The features selected from 3 components are used for downstream xgboost classification. In the cross validation for xgboost hyperparameter tuning, feature selection is re-calculated for each different fold of data. In the training of the final model using all training data. The selected features are computed again and saved as a filter. A total of 185 features are selected here. This filter will also be used to filter the test data, and the filtered test data will be used for the final evaluation of the model.
- For demonstration purpose we also did a splsda using the filtered features only, shown in figure 1C and 1E. {remind yourself what are figure 1C and 1E}. Although it seems like the groups are heavily overlapping in figure 1C, in the 3d plot (figure 1E) the separation is quite good. In performance evaluation of this splsda model, it also achieved overall classification error rate less than 20% (figure not shown). The percent variance explained by the features also increases to about 45%. {add your thoughts on why it is still not very high, although higher than before. This could have to do with the lenient feature selection thresholds and that HR+ and Triple Neg groups are difficult to separate - variance between them are harder to explain.} HR+ and Triple Neg are still difficult to classify using the linear classifier. Hence we chose to use a non-linear classifier like XGBoost.
- The part below explains results from xgboost hyperparameter optimization and evaluation of and results from the optimal parameters
- 5x10 nested cross validation was done to identify the optimal combination of XGBoost parameters. The optimal XGBoost parameters corresponding to are learning rate = 0.3, max_depth = 9, subsample = 0.9, colsample_bytree = 1.0. The max tree depth, row subsample ratio, and column subsample ratio are all relatively high. However, since the input features to XGBoost are already filtered, the risk of XGBoost overfitting to the data is not high.
- 5x10 nested cross validation was then done to evaluate the optimal parameters and calculate its performance metrics. 
- Table 1: Average confusion matrix from 5 x 10 nested cross validations
- Supplement Figure 1: histogram of overall classification error rates from 5 x 10 nested cross validations
- Supplement Figure 2: ROC plots from all 5 x 10 nested cross validations
- Table 1 shows that HER2+ were classified with 100% accuracy, HR+ 85.8%, and Triple Neg 75.3% in the cross validations. The mean overall classification error rate is 13.01% (Supplement Figure 1). The average AUCs are 1.00 for HER2+, 0.90 for HR+, and 0.89 for Triple Neg (Supplement Figure 2). 
- The part below explains SHAP analysis for the XGBoost model
- Figure 5: A-C are SHAP values of features for class HER2+, HR+, and Triple Neg. The top 15 features are displayed.
- SHAP values reflect how much the feature contributes to the prediction of the class. A higher absolute value means higher contribution. Figure 5A shows that the 17_35076296_35282086 feature alone contributes the prediction of HER2+ class. Figure 5B and 5C shows that the 17_35076296_35282086 feature is also the most important feature for predicting class HR+ and Triple_Neg. This observation agrees with our observation from SPLSDA that 17_35076296_35282086 is a very important feature. 
- Figure 6: SHAP dependence plot of 17_35076296_35282086 for predicted class HER2+ (A), HR+ (B), and Triple Neg (C).
- Supplement figure 2: force plot with 9 subplots for 3 samples from 3 classes. Supp. 2A-C are force plots for true class HER2+ and class prediction HER2+ (A), HR+ (B), and Triple Neg (C). Supp. 2D-E are force plots for true class HR+ and class prediction HER2+ (D), HR+ (E), and Triple Neg (F).  Supp. 2G-I are force plots for true class Triple Neg and class prediction HER2+ (G), HR+ (H), and Triple Neg (I). f(x) represents the baseline prediction from all the data. E(f(x)) represents the final prediction. Yellow arrows push the prediction towards the final prediction, which magenta arrows push the prediction away from it. The feature name, value, and SHAP value are labeled for the arrows {you should make the description more compact or move some information into the main text.}
- The x axes of figure 6 is the raw values of 17_35076296_35282086, in our case it only takes 4 values -1, 0, 1, 2 (loss, normal, gain, amplification). The y axis is the SHAP values of the samples. We observe that for HER2+ predictions, amplification (2) correspond to a higher SHAP value than the rest (-1, 0, 1), suggesting an increase in the copy number of this chromosomal region is related to HER2+ cancer subtype. While for HR+ and Triple Neg predictions, -1, 0, and 1 have similar levels of SHAP values, and 2 have different but lower SHAP values, reflecting that the distinction between amplification and the rest is also important in HR+ and Triple Neg predictions. 
- The SHAP values in Figure 6B and 6C are more spread out because 17_35076296_35282086 is not the only contributing feature. so its contribution varies depending on the other features of the sample. While for figure 6A, the SHAP values are the same for each value of 17_35076296_35282086 because it is the only feature contributing to HER2+ predictions. How the features collectively contribute to classification is visualized in force plots (Supplement figure 2) for 3 samples from 3 classes as examples. 
