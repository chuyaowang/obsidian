# ML of biomedical dataset

[0. VU B4TM](Machine%20Learning/VU%20Bioinformatics%20for%20Translational%20Medicine/0.%20VU%20B4TM.md)

## Feature selection and regularization

- **Feature selection**
	- Feature selection: reduce the number of features by removing irrelevant features
	- Need to identify important features
- **Regularization**
	- Prevent overfitting by penalizing model complexity
		- This may reduce the number of features in the process
- Both increases _interpretability_

### Why select features

- Classification
	- Simpler classifier: faster and more generalizable
	- Alleviate the curse of dimensionality
- Translational medicine
	- Identify biomarkers for diseases
	- Reduce the number of features to be measured in clinical use

### Types of features

- Continuous
	- ex. gene expression
- Count/integer
	- ex. CNV
- Ordinal: $\{low, mid, high\}$
	- tumor stages
- Categorical: $\{red, blue, green\}$
	- mutated/wild type
- [CNV](Machine%20Learning/VU%20Bioinformatics%20for%20Translational%20Medicine/1.%20VU%20B4TM%20Introduction.md#Copy%20number%20data%20from%20arrayCGH) can be count, ordinal, or categorical

### Regression

- ex. [Linear Regression](Machine%20Learning/Understanding%20Deep%20Learning/2.%20UDL%20Supervised%20Learning.md#Linear%20Regression)

### Feature selection strategies

- Manually choose features based on biological knowledge
- Filtering features based on relation with class labels
- Regularization: combined classification and selection (technically not feature selection)
- Advice: start with a simple classifier

#### Manually choose features

- Ex. profile how cell lines response to drugs. Want to understand what makes cells sensitive or insensitive to drug
- Clinical question: for people with BRAF mutant, should they get BRAF inhibitors?
- Cell line: cells grown in the lab
	- tested 926 GDSC1000 cancer cell lines against 265 drugs (IC50s)
- Data: 
	- features of the cell lines
		- mutations
		- copy number aberrations
		- gene expression profile
		- methylation
		- cancer type
	- Label
		- Drug sensitivity (IC50): the concentration of drug that kills 50% of cells
- Understand: why cells do or do not respond to a drug based on their molecular profile
- Feature selection: used TCGA database (cancer genomic database)
	- chose genes that are often mutated from TCGA
	- Why this approach: TCGA features are more relevant to real cancer rather than cells grown in a dish
- Goal of project: find biomarkers of drug response rather than prediction
	- Poor prediction performance but simple ANOVA identified biomarkers for drug response
	- The features may not explain everything but are still helpful
- Lessons learned:
	- Use prior knowledge for feature selection
	- Biological/clinical relevance for feature selection
	- Predictive performance is _not_ always the goal

#### Filter based methods

- Look at all features independently and how much they associate with the outcome
- Scoring association: correlation, mutual information, t statistic, F statistic, p value, tree importance statistic, etc.
- Rank based on the score
- Advantage: easy to interpret, can provide some insight into the disease markers
- A simple approach:
	- Simple statistical test or metric (ex. weights in logistic regression) to capture class separation using class labels
	- Ranks genes according to score
	- Choose the top $n$ ranking genes
	- thought: PLSDA for feature selection with VIP values?
- Problem:
	- **redundancy**: high correlation between features. they do not contribute independent information
	- **interactions** among features cannot be incorporated: since features are considered independently. However, some filtering methods are smarter than others
	- **Classifier** has no say in what features should be used: some scores may be more suitable for a specific classifier

### Regularization

- Feature selection while training a classifier
- Example continued: predict IC50 from gene expression

#### Multivariate linear regression

- Multivariate linear regression: $IC_{50Cell A}=w_{1}\cdot Gene_{1,cell A}+w_{2}\cdot Gene_{2,cell A}+\dots + w_{N}\cdot Gene_{N,cell A}+\epsilon_{cell A}$
	- Calculate for all cells
	- Minimize sum of square error ($\epsilon^{2}$)
- Find the feature weights that best describe the training data
- Problem: more columns than rows. more unknowns than equations. There is no unique solution.
- Solution: regularization

#### Example: regularization for linear regression

- Curve fitting: $$
y(x,w)= w_{0}+w_{1}x+w_{2}x^{2}+\dots +w_{M}x^M=\sum_{j=0}^M w_{j}x^j
$$
- As $M$ increases, the curve becomes more complex
- Error function before regularization: $E=\frac{1}{2} \sum_{i}^N \{y(x_{n},w)-t_{n}\}^{2}$
- Regularization: penalty term to error function to penalize model complexity: $$E=\frac{1}{2} \sum_{i}^N \{y(x_{n},w)-t_{n}\}^{2}+\frac{\lambda}{2}|w|^{2}$$
- $\lambda$ tunes regularization strength

#### Multivariate linear regression continued

- Minimize $$
\epsilon^{2}+\lambda[(w_{1}^{2}+\dots+w_{N}^{2})+\alpha(|w_{1}|+\dots+|w_{N}|)]
$$
- Penalizes using too many features and high coefficients
	- Penalize square term: make value lower but not 0; square of small value is smaller
	- Penalize absolute value: make coefficients becomes 0
- This approach is called elastic net (r package `glmnet`)
- How to pick $\alpha$ and $\beta$: cross validation

## Cross validation

> Previous years assignment results: everybody beats random classifier but almost everybody overestimated their performance

- Cross validation is used for:
	- Choose hyperparameters
	- Evaluate model performance

### Choose hyperparameters

![](Media/Pasted%20image%2020250408181615.png)
- Plot: error vs. hyperparameter
	- example uses $\lambda$ for regularization
- Do 5 fold CV for different $\lambda$s. Take the average of the errors in each CV.
- Left end: overfitting on training data and bad performance in validation data; lowest regularization
- Right end: model too simple and bad performance in validation data; highest regularization

### Question: is feature selection part of the training/model selection? Why or why not?

- It is a part of model training. Everything that uses _labels_ is part of model training.
- Feature selection needs to happen **within** cross-validation loop.

### Can we do model or hyperparameter selection and performance evaluation on the same data? Why or why not?

- No
- For example, when selecting $\lambda$, test 100 different $\lambda$s. This gives a distribution of errors. Some parameters can perform particularly well just by chance for a specific fold of data even if it is equally good as other $\lambda$s.
	- More severe when there are many hyperparameters
	- This is also a form of overfitting
- Need to separate model/hyper parameter selection and performance evaluation
- Solution: nested cross-validation loop
	- Outer sampling: for performance evaluation
		- ex. 5 fold CV, 1-4 partitions
	- Inner sampling: happens within the 4 partition of outer sampling; used for hyperparameter tuning
	- Outer sampling uses the tuned parameters from inner sampling

![](Media/Pasted%20image%2020250408183717.png)

### Questions to consider for CATS

- Remember that feature selection is also a form of learning
- Would you need to do feature selection on your training set or on your entire set for estimating the accuracy of your method?
- Choosing a classification method out of many is also learning
- Tip: start thinking about a good CV scheme sooner than later

## Some ML methods

- Nearest mean: distance to the nearest mean
- KNN: classify based on K closest samples; K is a hyperparameter
- Logistic regression: $y(x)=\frac{1}{1+e^{-wx-b}}$
- Random forest: ensemble of decision trees
	- [9. VU BDA Random Forest](Machine%20Learning/VU%20Biosystems%20Data%20Analysis/9.%20VU%20BDA%20Random%20Forest.md)
- SVM
- DL

## Recap

- Number of features > number of samples means feature selection or regularization are necessary
- Feature selection and hyperparameter tuning are part of model training
- Cannot do model selection and performance evaluation on the same data $\Rightarrow$ nested cross validation
