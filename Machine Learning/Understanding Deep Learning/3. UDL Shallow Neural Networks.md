[0. Understanding Deep Learning](0.%20Understanding%20Deep%20Learning.md)
# Shallow Neural Networks

## An Example

- A neural network with ten parameters $\boldsymbol{\phi}=\{\phi_0,\phi_1,\phi_2,\phi_3,\theta_{10},\theta_{11},\theta_{20},\theta_{21},\theta_{30},\theta_{31}\}$:
$$
\begin{aligned}
y
&\quad
=\quad\mathrm{f}[x,\phi]
\\
&\quad
=\quad\phi_0+\phi_1\mathrm{a}[\theta_{10}+\theta_{11}x]+\phi_2\mathrm{a}[\theta_{20}+\theta_{21}x]+\phi_3\mathrm{a}[\theta_{30}+\theta_{31}x]
\\
\end{aligned}$$
- This neural net models a piece-wise linear function with up to 4 pieces.
- $\theta$: parameters for linear functions of the input data.
- $\mathbf{a}[\bullet]$: the activation function.
	- The *ReLU* function is chosen here. It turns all negative values of the linear function to 0: $$\operatorname{a}[z]=\operatorname{ReLU}[z]=\begin{cases}0\quad&z<0\\z\quad&z\geq0&\end{cases}.$$
	- Each result of the activation function is called a hidden unit. i.e.: $$h_1 = a[\theta_{10}+\theta_{11}x]$$
- $\phi$: weights for the hidden units. $\phi_0$ is an offset amount that controls the height of the function.
- Each region in the final piece-wise function are affected by different *activation patterns*, which are combinations of hidden units.
	- Active: part linear function not clipped by ReLU.
	- Inactive: part of linear function clipped by ReLU.
- Each hidden unit determines one joint of the function. So with 3 hidden units, there are 4 pieces of linear functions. The last piece is either 0 or a sum of slopes from the other regions.
- Depicting the neural net:![](Pasted%20image%2020230822170029.png)
	- a: with the intercept parameters.
	- b: without the intercept parameters for better readability.

## [The Universal Approximation Theorem](The%20Universal%20Approximation%20Theorem.md)

- Generalization of a shallow neural net: $$y=\phi_0+\sum_{d=1}^D\phi_dh_d$$, where D is the number of hidden units.
- This shallow neural net has D joints and D+1 linear regions.
- With enough hidden units, a shallow network can describe any continuous 1D function defined on a compact subset of the real line to arbitrary precision.

## General Form of Shallow Neural Networks

- The general form maps multiple inputs to multiple outputs.
- For multiple inputs, there is one slope parameter for each input in each hidden unit. i.e.: $$\begin{aligned}h_1&\quad=\quad\mathrm{a}[\theta_{10}+\theta_{11}x_1+\theta_{12}x_2]\\h_2&\quad=\quad\mathrm{a}[\theta_{20}+\theta_{21}x_1+\theta_{22}x_2]\\h_3&\quad=\quad\mathrm{a}[\theta_{30}+\theta_{31}x_1+\theta_{32}x_2]\end{aligned}$$
- For multiple outputs, there is one set of weights for each output variable combining the same hidden units. i.e.: $$\begin{aligned}y_1&\quad=\quad\phi_{10}+\phi_{11}h_1+\phi_{12}h_2+\phi_{13}h_3+\phi_{14}h_4\\y_2&\quad=\quad\phi_{20}+\phi_{21}h_1+\phi_{22}h_2+\phi_{23}h_3+\phi_{24}h_4.\end{aligned}$$
- General form that maps a multi-dimensional input $\mathbf{x}\in\mathbb{R}^{D_i}$ to a multi-dimensional output $\mathbf{y}\in\mathbb{R}^{D_o}$: $$y_j=\phi_{j0}+\sum_{d=1}^D\phi_{jd}h_d,$$where $$h_d=\text{a}\left[\theta_{d0}+\sum_{i=1}^{D_i}\theta_{di}x_i\right]$$

## Terminologies

![](Pasted%20image%2020230822193157.png)

- Input layer: the leftmost layer taking in the input;
- Output layer: the rightmost layer calculating the output;
- Hidden layer: the layer(s) in between;
- Neurons: each hidden unit;
- Pre-activations: inputs to the hidden layer before activation;
- Activations: outputs from activation functions;
- Multi-layer perceptron: any neural network with at least one hidden layer;
- Shallow neural network: networks with one hidden layer;
- Deep neural network: networks with multiple hidden layers;
- Acyclic graph: a graph with no loops;
- Feed-forward network: neural networks in which the connections form an acyclic graph;
- Fully connected network: every element in one layer connects to every element in the next;
- Network weights: $\boldsymbol\phi$  of the network, represented by each connection.