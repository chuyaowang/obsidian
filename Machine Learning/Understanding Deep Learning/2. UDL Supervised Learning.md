[0. Understanding Deep Learning](0.%20Understanding%20Deep%20Learning.md)
# Supervised Learning

- A *model* is a mathematical equation that computes the output from the input.
- *Inference* is the process of calculating the output.
- *Parameters* specifies the relationship that the model describes.

## Overview

- A model: $$y = f[x,\phi]$$
- It takes $x$ and the parameters of the model $\phi$ and return $y$.
- Training: use pairs of input and output $\{x_i,y_i\}$ to find the parameters that map $y$ to $x$.
- *Loss*: The degree of mismatch between predicted and actual $y$. The loss is a function of the parameters as $L[\phi]$.
- Evaluation: use a *test* dataset to see how well the model generates to new data.

## Linear Regression

- The model:$$y = \phi_0+\phi_1x$$
- The *least squares* loss is the sum of all squared errors of each data point: $$L[\boldsymbol\phi] = \sum_{i=1}^I\left(\phi_0+\phi_1x_i-y_i\right)^2$$
- The loss $L[\boldsymbol\phi]$ can be visualized as a surface or a heat map, since there are only 2 parameters.
- The goal is to find parameters $\hat{\boldsymbol{\phi}}$ that minimize the loss function:
$$
\begin{aligned}
\hat{\boldsymbol{\phi}}&
\quad=\quad\underset{\boldsymbol{\phi}}{\operatorname*{\mathrm{argmin}}}\begin{bmatrix}\mathrm{L}[\boldsymbol{\phi}]\end{bmatrix}
\\
&\quad=\quad\underset{\boldsymbol{\phi}}{\operatorname*{\mathrm{argmin}}}\left[\sum_{i=1}^I\left(\text{f}[x_i,\boldsymbol{\phi}]-y_i\right)^2\right]
\\
&\quad=\quad\underset{\boldsymbol{\phi}}{\operatorname*{\mathrm{argmin}}}\left[\sum_{i=1}^I\left(\phi_0+\phi_1x_i-y_i\right)^2\right]
\end{aligned}$$
- Training: *gradient descent* can be used to find the parameters with the minimal loss.
	- For linear regression, a closed form solution for optimal parameters can be found by setting the gradients to 0 and solving for $\phi_0$ and $\phi_1$.
- Testing:
	- Test the model on test data.
	- [Overfitting](Overfitting.md): the model fits training data too well and does not generalize well for testing data.
	- Underfitting: the model does not capture the relationship in the data. 

[Interpreting linear regression as vector projection](https://few.vu.nl/~molenaar/courses/Math/chapters/07_orthogonality.html#independence-norm-and-orthogonality)