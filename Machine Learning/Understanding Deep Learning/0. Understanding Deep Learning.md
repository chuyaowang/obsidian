# Book: Understanding Deep Learning

[Notebooks, slides, blogs](https://udlbook.github.io/udlbook/)
[Github, get new version of book](https://github.com/udlbook/udlbook)
[Current book](UnderstandingDeepLearning_08_28_24_C.pdf)

## Introduction

Artificial Intelligence: building systems that simulate intelligent behavior based on logic (propositional logic), search (Dijkstra), and probabilistic reasoning (Bayesian models).

Machine learning: a sub-field of AI that involves fitting mathematical models to observed data.

Deep learning: fitting deep neural networks to data.

### Supervised Learning

Define a mapping from input data to an output prediction.

- Regression and Classification
	- Regression: predicts a value.
	- Classification: predicts a label.
- Input
	- Can be many types including tabular data, words, sounds, images, etc.
- Model
	- The model represents a family of equations mapping the input to the output.
	- Training the model means searching through the family of equations to find one that explains the data.
- Deep neural networks
	- A type of model that can represent a broad family of relationships
- Structured outputs
	- Deep learning can be trained to generate structured outputs like images from text, semantic segmentation, language translation, speech transcription, etc.

### Unsupervised Learning

The model is trained without an output. The goal is to describe and understand the structure of the data.

- Generative models
	- Unsupervised models that can be used to generate new data.
	- Some learns the probability distribution of the input data and sample from it. Others learn a mechanism that generates new samples.
	- Examples: image and text generation, image inpainting, text completion.
- **Latent variables**
	- The amount of important variables for the distribution of the data can be less than the number of observed variables. These variables are called latent variables.
	- For example, the probability distribution of a human facial expression photo can be described by the state of the 42 muscles on the human face.
	- The role of deep learning is mapping the observed variable to latent variables.
	- The probability distributions of the latent variables are often designed to be simple, like a Gaussian distribution. This way, new data can be sampled easily from learned distributions.
	- By learning the latent representations of two samples, we can interpolate between these distributions to learn how one sample transforms into another.
- Connecting supervised to unsupervised learning
	- Supervised learning can be made easier by learning the mapping between latent variables of the input to the latent variables of the output.
	- Comparing to learning with observed variables, there are several advantages:
		- Need fewer data to train the model since the latent variables are lower dimensional.
		- More likely to generate meaningful output.
		- Multiple outputs can be generated by introducing randomness with noise.

### Reinforcement Learning

Trains an agent to take actions. The actions of the agent will change the state of the system but not necessarily in a deterministic way. The action will also produce rewards or punishments. The goal is to train the agent to choose the action that leads to high rewards.

Temporal credit assignment problem: the reward may not take place immediately. 

Exploration vs. exploitation: should the agent use learned methods to get modest rewards or try new actions?

Deep learning can be used to map observed state to an action. This is known as a *policy network*. The mapping is known as a *policy*.

## Supervised Learning

- A *model* is a mathematical equation that computes the output from the input.
- *Inference* is the process of calculating the output.
- *Parameters* specifies the relationship that the model describes.

### Overview

- A model: $$y = f[x,\phi]$$
- It takes $x$ and the parameters of the model $\phi$ and return $y$.
- Training: use pairs of input and output $\{x_i,y_i\}$ to find the parameters that map $y$ to $x$.
- *Loss*: The degree of mismatch between predicted and actual $y$. The loss is a function of the parameters as $L[\phi]$.
- Evaluation: use a *test* dataset to see how well the model generates to new data.

### Linear Regression

- The model:$$y = \phi_0+\phi_1x$$
- The *least squares* loss is the sum of all squared errors of each data point: $$L[\boldsymbol\phi] = \sum_{i=1}^I\left(\phi_0+\phi_1x_i-y_i\right)^2$$
- The loss $L[\boldsymbol\phi]$ can be visualized as a surface or a heat map, since there are only 2 parameters.
- The goal is to find parameters $\hat{\boldsymbol{\phi}}$ that minimize the loss function:
$$
\begin{aligned}
\hat{\boldsymbol{\phi}}&
\quad=\quad\underset{\boldsymbol{\phi}}{\operatorname*{\mathrm{argmin}}}\begin{bmatrix}\mathrm{L}[\boldsymbol{\phi}]\end{bmatrix}
\\
&\quad=\quad\underset{\boldsymbol{\phi}}{\operatorname*{\mathrm{argmin}}}\left[\sum_{i=1}^I\left(\text{f}[x_i,\boldsymbol{\phi}]-y_i\right)^2\right]
\\
&\quad=\quad\underset{\boldsymbol{\phi}}{\operatorname*{\mathrm{argmin}}}\left[\sum_{i=1}^I\left(\phi_0+\phi_1x_i-y_i\right)^2\right]
\end{aligned}$$
- Training: *gradient descent* can be used to find the parameters with the minimal loss.
	- For linear regression, a closed form solution for optimal parameters can be found by setting the gradients to 0 and solving for $\phi_0$ and $\phi_1$.
- Testing:
	- Test the model on test data.
	- [Overfitting](Overfitting.md): the model fits training data too well and does not generalize well for testing data.
	- Underfitting: the model does not capture the relationship in the data. 

## Shallow Neural Networks

### An Example

- A neural network with ten parameters $\boldsymbol{\phi}=\{\phi_0,\phi_1,\phi_2,\phi_3,\theta_{10},\theta_{11},\theta_{20},\theta_{21},\theta_{30},\theta_{31}\}$:
$$
\begin{aligned}
y
&\quad
=\quad\mathrm{f}[x,\phi]
\\
&\quad
=\quad\phi_0+\phi_1\mathrm{a}[\theta_{10}+\theta_{11}x]+\phi_2\mathrm{a}[\theta_{20}+\theta_{21}x]+\phi_3\mathrm{a}[\theta_{30}+\theta_{31}x]
\\
\end{aligned}$$
- This neural net models a piece-wise linear function with up to 4 pieces.
- $\theta$: parameters for linear functions of the input data.
- $\mathbf{a}[\bullet]$: the activation function.
	- The *ReLU* function is chosen here. It turns all negative values of the linear function to 0: $$\operatorname{a}[z]=\operatorname{ReLU}[z]=\begin{cases}0\quad&z<0\\z\quad&z\geq0&\end{cases}.$$
	- Each result of the activation function is called a hidden unit. i.e.: $$h_1 = a[\theta_{10}+\theta_{11}x]$$
- $\phi$: weights for the hidden units. $\phi_0$ is an offset amount that controls the height of the function.
- Each region in the final piece-wise function are affected by different *activation patterns*, which are combinations of hidden units.
	- Active: part linear function not clipped by ReLU.
	- Inactive: part of linear function clipped by ReLU.
- Each hidden unit determines one joint of the function. So with 3 hidden units, there are 4 pieces of linear functions. The last piece is either 0 or a sum of slopes from the other regions.
- Depicting the neural net:![](Pasted%20image%2020230822170029.png)
	- a: with the intercept parameters.
	- b: without the intercept parameters for better readability.

### [The Universal Approximation Theorem](The%20Universal%20Approximation%20Theorem.md)

- Generalization of a shallow neural net: $$y=\phi_0+\sum_{d=1}^D\phi_dh_d$$, where D is the number of hidden units.
- This shallow neural net has D joints and D+1 linear regions.
- With enough hidden units, a shallow network can describe any continuous 1D function defined on a compact subset of the real line to arbitrary precision.

### General Form of Shallow Neural Networks

- The general form maps multiple inputs to multiple outputs.
- For multiple inputs, there is one slope parameter for each input in each hidden unit. i.e.: $$\begin{aligned}h_1&\quad=\quad\mathrm{a}[\theta_{10}+\theta_{11}x_1+\theta_{12}x_2]\\h_2&\quad=\quad\mathrm{a}[\theta_{20}+\theta_{21}x_1+\theta_{22}x_2]\\h_3&\quad=\quad\mathrm{a}[\theta_{30}+\theta_{31}x_1+\theta_{32}x_2]\end{aligned}$$
- For multiple outputs, there is one set of weights for each output variable combining the same hidden units. i.e.: $$\begin{aligned}y_1&\quad=\quad\phi_{10}+\phi_{11}h_1+\phi_{12}h_2+\phi_{13}h_3+\phi_{14}h_4\\y_2&\quad=\quad\phi_{20}+\phi_{21}h_1+\phi_{22}h_2+\phi_{23}h_3+\phi_{24}h_4.\end{aligned}$$
- General form that maps a multi-dimensional input $\mathbf{x}\in\mathbb{R}^{D_i}$ to a multi-dimensional output $\mathbf{y}\in\mathbb{R}^{D_o}$: $$y_j=\phi_{j0}+\sum_{d=1}^D\phi_{jd}h_d,$$where $$h_d=\text{a}\left[\theta_{d0}+\sum_{i=1}^{D_i}\theta_{di}x_i\right]$$

### Terminologies

![](Pasted%20image%2020230822193157.png)

- Input layer: the leftmost layer taking in the input;
- Output layer: the rightmost layer calculating the output;
- Hidden layer: the layer(s) in between;
- Neurons: each hidden unit;
- Pre-activations: inputs to the hidden layer before activation;
- Activations: outputs from activation functions;
- Multi-layer perceptron: any neural network with at least one hidden layer;
- Shallow neural network: networks with one hidden layer;
- Deep neural network: networks with multiple hidden layers;
- Acyclic graph: a graph with no loops;
- Feed-forward network: neural networks in which the connections form an acyclic graph;
- Fully connected network: every element in one layer connects to every element in the next;
- Network weights: $\boldsymbol\phi$  of the network, represented by each connection.




