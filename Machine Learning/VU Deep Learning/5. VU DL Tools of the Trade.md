# Tools of the trade

[0. VU Deep Learning](0.%20VU%20Deep%20Learning.md)

## Splitting the data

- Test data: reserved to test the final performance of the model
- Validation data: develop the model and tune hyperparameters (learning rate, batch size, etc.)
- Training data: whatever that is left

### Size of test data is more important than size of training data

- The greater the size, the more confidence we have about the accuracy it estimates
- 10000 instance for test is ideal, 1000 is good, below is not good
- validation should be half the size of test set
- if your dataset is too small:
	- consider not using machine/deep learning
	- find lots of unlabeled data: self/semi-supervised learning
	- for evaluation: combined 5 by 2 cross validation F testing

### Do not use test set more than once

- Don't touch it until the final model is chosen

### Test set leakage

- information from test or validation set is leaked into training set. This makes the model have a higher than normal performance in validation and test data.
- Examples:
	- spam detection: emails shuffled in time dimension
		- if emails are shuffled before making the split, emails from the future can be inside the training set. Then when the model encounters emails from the future in the test set, it will have a higher than normal performance.
		- when it encounters new data in production, it will not generalize well.
	- link prediction: graphs with inverse links
		- if a link is in the training set and its inverse is in the test set, the information is leaked.
	- preprocessing before splitting: normalization and running averages over the whole data.

## Debugging the model

### Neural network fail at runtime

- shape errors: shape of tensors do not match for multiplication

### Neural network fail silently

- especially due to broadcasting

### May not fail at all

- runs but does not do what we want it to do

### tricks

- `assert` keyword: assert followed by condition followed by a message:
	- `assert my_tensor.size()==(b,c,h,w)`
	- `assert not contains_nan(x), 'tensor x contains NaN'`
	- `assert len(x)==n,"f'tensor x has dim {len(x)}, expected {n}'"`
- Expect assert keyword to be turned off in production settings
	- do it by running the script with `-O` flag
	- assert won't impact production performance
	- don't use it to validate user input

### broadcasting

- numpy and pytorch
- apply element-wise operations to tensors of different sizes
- be careful what it gives
- Example: ![](Pasted%20image%2020241201204656.png)
	- element-wise multiplication of a vector (16,) and a vector (16,1)
	- aligns two vectors by the rightmost dimension
	- broadcast so the dimensions match
	- element-wise multiplication
- to avoid
	- add the singleton dimensions yourself
	- `keepdim`: use when doing an operation that can remove a dimension
		- `normalized = x/x.sum(dim=1,keepdim=True)`
	- open each method by getting the shapes of the inputs: `b,c,h,w=input.size()`
		- add copious asserts especially for tensor shapes
			- assert rowsums.size() = (b,c,h,1)

### Memory leak

- when keeping a running loss (loss for the whole epoch) in the loop, adding the loss node directly causes all the computational graphs linked to this loss node to be kept in memory. As the epoch number increases, eventually the computational graph fills up the memory

``` python
for e in range(epochs) :
	running_loss = 0.0
	for x, t in dataset:
		opt. zero_grad()
		y = model(x)
		l= loss(y, t)
		l. backward()
		opt.step()
		running_loss += l
	print(f'epoch {e} total loss: {running_loss}')
```

- Alternatively, track the running loss with `running_loss += l.item()` when the loss is a scalar. 
- Use `x.detach()` and `x.data` for tensor values.

### NaN Loss

- When the loss becomes NaN (not a number). Happens when somewhere in the network some value has become either NaN or infinite. Then it quickly makes everywhere else NaN.
- Usually first noticed at loss, but loss is not always where it started.
- learning rate is too high
	- try very low (1e-12) or zero learning rate
	- if these are also NaN it's a bug.
- If not, there are 2 options:
	- it's a bug that shows only for some parameters
	- the network is fine so long as you keep the learning rate reasonable
- localize problem with asserts
	- `assert not x.isnan().any()`
	- `assert not x.isinf().any()`

### No learning

- The model does not learn. The loss stays at the same.
- Try different learning rates: 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3...
	- If it happens for all learning rates, it's probably a bug
	- check what gradients pytorch is computing
	- to let pytorch keep gradients it no longer needs for debugging, use `x.retain_grad()` in the forward pass to a tensor.
``` python
x.retain_grad()
loss.backward()
print(x.grad.min(),x.grad.mean(),x.grad.max())
```
- Gradient is None: backpropagation did not reach it. The computational graph is somehow disconnected (perhaps you used a non-differentiable operation somewhere).
- Gradient is 0: backpropagation reached the node, but the gradient died out. This can happen, for instance if you have ReLU activations somewhere with all inputs always negative, or a sigmoid activation with very large negative or positive inputs.