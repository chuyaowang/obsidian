# Sequential Data

[0. VU Deep Learning](0.%20VU%20Deep%20Learning.md)
[Slides](https://dlvu.github.io/sequences/)

## Sequence data

- times series
- 2 dimensional series
- symbolic (categorical) in a sequence: words and sentences

### Single vs Multiple Sequences

- scam detection (see also [Spam filter](4.%20VU%20FoB%20-%20Machine%20Learning.md#Spam%20filter)): predict from a sequence where each email, which is a sequence of letters, is part of the whole sequence. No features, only words.
- Given a single sequence, predict the next signal.

### Walk-forward Validation

- If the data is arranged in time, and you shuffle the data in training, this could move future data behind past data. This will make the model see the future data that it is supposed to predict during training.
- Data need to be arranged in time.
- The solution: walk-forward validation. Training the model in small segments and use the segment immediately after for validation. All while keeping the time order of the data.

## Sequences in DL

Sequence to sequence

### Sequence to sequence layer

- input: length t sequence of vectors.
- output: length t sequence of vectors.
	- input/output dimension can be different, but t must be the same.
- the same layer (same _weight_) can be applied to sequences of different lengths.

Fully connected layer:
- each input node connected to each output node.
- problem: when the number of nodes change, the number of weights change.
MLP applied to each input:
- each input connected to each output
- same weights used over the connections
- new input and output can be connected in the same way
- problem: time information is not used. Output does not depend on the past or future input.
Convolution layer
- each output is connected to present, past, and future inputs via a convolution layer

### Looking forward and back

- can be connected to future and past events or only future or only past.

## Preparing data

### Representing discrete inputs

#### One hot encoding

- Use a vector with all zeros but 1 at the input letter 
- problem: vocabulary too large, [Sparsity](Sparsity.md)

#### Embedding vectors

See also [Graph Embedding](Graph%20Embedding.md)

- Initialize a random vector for each letter in the input.
- map the vectors in the order of the original data.
- train the values of the embeddings

### Batching sequences

Problem: sequences are of different length

Solution:
- padding the short sequences with zeros

Lengthwise sorting: minimize the amount of padding needed when choosing small batches.

Variable batch size: adjust batch size depending on sequence length to balance memory use.

Something skipped here.

## Model configurations

- sequence to sequence: POS tagging, machine translation, robot control, generation
- sequence to label: classification, regression
- label to sequence: generative models
- label+sequence to sequence: teacher forcing

### Sequence to sequence

- Stack [Sequence to sequence layer](#Sequence%20to%20sequence%20layer)s. Predict sequence of labels from sequence of words.
- Autoregressive modeling: predict the next letter in the sequence.

### Sequence to label

- Global pooling: stacking sequence to sequence layers and in the end pool the sequences to predict a label.
	- The pooling part need to scale to different sequence lengths
	- Global unit: choose one out of the output sequence to represent the whole sequence
	- other methods

## Recurrent Neural Network

### Elman network

The hidden layer from the previous element in the sequence is appended to the input and feeds into the network.

How to train it?
- At time t, the network of t-1 has disappeared. So you cannot update the weights to get a better hidden layer.
- Solution: network unrolling. Then do back propagation.
- Weights will get different gradients from networks in each time point. Sum all the gradients.

### Features of RNNS

- sequence to sequence layers
- RNNs are causal: only look backward in time
- Potentially unbounded memory; convolution has a limited number of step forward and back; RNNs can look all the way to the back.

## LSTM

Long short-term memory

### The problem of long-term dependence

[Long Range Dependency](Long%20Range%20Dependency.md)
- In a sentence, relevant information may be separated by a long distance.
- Need to keep long term memory of relevant information, and do not memorize irrelevant information along the way.

## Examples

- Shakespere dramas: lack long term coherence: characters in the dram do not appear again.

