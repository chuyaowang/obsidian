# Self Attention

[0. VU Deep Learning](Machine%20Learning/VU%20Deep%20Learning/0.%20VU%20Deep%20Learning.md)

## Recap: sequence to sequence layer

[4. VU DL Sequential Data](Machine%20Learning/VU%20Deep%20Learning/4.%20VU%20DL%20Sequential%20Data.md)

- inputs a sequence, outputs a sequence
- can handle sequences of different lengths
- versatile inputs and outputs
	- autoregressive learning: predict the next character in the swequence
- casual or non-causal: whether the model sees future data during training
- RNN: 
	- in principle look indefinitely far into the sequence
	- drawback: sequential processing
- CNN:
	- finite memory of context
	- computation can be done in parallel
- Self-attention:
	- parallel computation
	- indefinite memory

## Simple self attention



## Complete self attention