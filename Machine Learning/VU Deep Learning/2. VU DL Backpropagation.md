# Backpropagation

[0. VU Deep Learning](0.%20VU%20Deep%20Learning.md)
[Slides](https://dlvu.github.io/backpropagation/)
## Backpropagation

Backpropagation algorithms find the gradient of complex, multilayer functions.

- Scalar backpropagation
- Tensor backpropagation
- Automatic differentiation (autograd)

## Scalar backpropagation

- The symbolic approach: solve the gradient from the function directly using symbolic math softwares like wolfram alpha, but does not work (out of memory) when the number of variables increase;
- The numerical approach: do not compute the gradient from the function, only take two numbers from the function and compute their rate of change;
- The middle ground: work out part of it symbolically, connect these parts in a numeric computation; _the chain rule: $$\frac{\delta f(g(x))}{\delta x}=\frac{\delta f(g(x))}{\delta g(x)}\frac{\delta g(x)}{\delta x}$$
- Computation graph: illustrates the chain rule visually
- Multi-variate chain rule: sum all the paths on the computation graph: $$\frac{\delta f}{\delta x}=\sum_i \frac{\delta f}{\delta g_i}\frac{\delta g_i}{\delta x}$$
### Example of chain rule

$$\begin{aligned}f(x)&=\frac{2}{\sin(e^{-x})}\\a&=-x\\b&=e^a\\c&=\sin(b)\\d&=\frac{2}{c}\\\frac{\delta f}{\delta x}&=\frac{\delta d}{\delta c}\frac{\delta c}{\delta b}\frac{\delta b}{\delta a}\frac{\delta a}{\delta x}\end{aligned} $$
### Backpropagation

- breaks down the computation up into a sequence of operations
- work out the _local derivatives_ symbolically
- compute the _global derivative_ numerically
- much more accurate than finite differences
- much faster than symbolic math

## Automatic differentiation

[Example backpropagation of a linear layer](https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html)

