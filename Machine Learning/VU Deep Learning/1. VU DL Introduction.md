# Introduction

[0. VU Deep Learning](0.%20VU%20Deep%20Learning.md)
[Slides](https://dlvu.github.io/introduction/)
## Neural Network

Inspired by the human neuronal network

### The perceptron

Basically [Linear Regression](2.%20UDL%20Supervised%20Learning.md#Linear%20Regression).
Multiply each input by a weight and sum them with a bias:
$$y = w_1x_1+w_2x_2+b$$

### Compositing neurons

[3. UDL Shallow Neural Networks](3.%20UDL%20Shallow%20Neural%20Networks.md)

- Multiple layers of neurons can be added.
- However, this does not make the network able to model non-linear functions.
- The solution is to use an **activation function**, $\sigma$.
$$y=\sigma(w_1x_1+w_2x_2+b)$$

### Activation function

- Logistic sigmoid: takes the range of numbers from negative to positive infinity and squishes them down to the interval between 0 and 1.
- Linear rectifier, or ReLU: sets every negative input to zero, and keeps everything else the same.
- Linear activation: not using an activation function.

### Multi-layer perceptron

![](Pasted%20image%2020230822170029.png)

- Using the activation functions, we can arrange single neurons into **neural networks**
- Also called feedforward network.
- Important characteristics:
	- There are **no cycles**, the network “feeds forward” from input to output.  
	- Nodes in the same layer are not connected to each other, or to any other layer than the previous one.  
	- Each layer is **fully connected** to the previous layer, every node in one layer connects to every node in the layer before it.
- The first layer: feature extractor. Like how features are extracted in [PCA](Linear%20Dimension%20Reduction.md#PCA).
- The second layer: performing a regression on the extracted features.

### Loss function

- To train the model, need to define a loss function.
- Training is the process of minimizing the loss.
- Example: $L = (\hat{y}-y)^2$ , squared error.
- Vector of all weights is usually called $\theta$.
- Model space: assume there are only two weights, all possible combinations of them define the **model space**
- Loss surface: each combination of the two weights corresponds to a loss. These losses can be plotted as a surface, called the loss surface.
- When minimizing the loss, we find the low point in this surface.
- Minimize the loss: $$\operatorname*{\mathrm{argmin}}_\boldsymbol{\theta} \operatorname{Loss}_{\mathrm{data}}(\theta)$$

### Gradient descent

- Derivative: slope of the tangent line of a function
- Partial derivative
- Gradient

## Classification and Regression

### Binary classification

- Use a sigmoid activation to transform the output node to between 0 and 1.
- The output is interpreted as the probability of being the positive class. The probability of being in the negative class is 1-output.
- Loss function: since the output is binary, a different loss function is needed:
	- should be a low value if the probability of the true class is close to one, and high if the probability of the true class is low.
	- the **log loss**: the negative log of the probability of the true class, also known as **cross-entropy loss**
$$\text{loss}=-\log\text{p(t|x) with p(t|x)}\begin{cases}\text{y}&\text{if t=pos}\\1-\text{y}&\text{if t=neg}\end{cases}$$

### Multiple classification

- Need multiple output nodes to achieve multiple classification.
- Scale all outputs to sum to 1.

### Other loss functions


## The first principles

[MLE vs. Bayesian Parameter Estimation](MLE%20vs.%20Bayesian%20Parameter%20Estimation.md)

### Maximum likelihood principle

Choose the parameters that make the probability of observing the data the highest.

## Autoencoder

