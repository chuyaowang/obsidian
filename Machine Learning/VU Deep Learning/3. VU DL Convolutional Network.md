# Convolutional Network

[0. VU Deep Learning](0.%20VU%20Deep%20Learning.md)
[Slides](https://dlvu.github.io/cnns/)

## Computer Vision

- Image labeling
- Image segmentation

### Getting features from images

- Red, blue, green channels.
- Color intensity of pixels.
- Problem: too many features this way, too many weights if we use [Multi-layer perceptron](1.%20VU%20DL%20Introduction.md#Multi-layer%20perceptron) (MLP).
- Also, the MLP does not use the locality of the pixels, treating them as independent.

## Sound signal

- 1D with amplitude with respect to time.
- Task: classify music genre, does the user like it, beat of the music, noise removal, generate audio for lyrics, etc.
- Manual feature extraction:
	- the traditional way.
	- use filters to extract features.
	- problem: noise, variations; the filters must be redesigned for each scenario.
- The DL approach uses the model to extract features and make predictions.

### Use MLP

- Lots of weights, does not use the order of the inputs, need a lot of training data.

### Filters

- The features in the sound wave are not independent.
- Closer features are more important than distant ones.

#### Silence filter

- Detect when there is a silence in the audio.
- Take the silent regions in the audio, train an MLP to classify it as silent vs. not silent.
- Takes a weighted sum of the signal values and fit into sigmoid to make prediction
- This filter can act as a feature extractor.
- It can be used at all locations to detect silence.

### Convolving the filter with the signal

- Sliding the filter along the data, called **convolving** the filter with the signal.
- The operation is called a **convolution operator**.
- The output of the convolution operator is itself again a vector.
- A time shift in the input causes the same shift in the output: convolution is **translation equivariant**.
- This operation can be understood as **a small MLP** that wanders across the input signal.
- In practice, the convolution kernels are learned.

## 1D convolution

Mathematical definition:
For a 1D input sequence
$$\mathrm{h(t)}=\mathrm{(x*k)}(t)=\sum^m_{\tau=-m}\mathrm{x}(t-\tau)\cdot\mathrm{k}(\tau)$$

### Learn the filter k

- The weights of the filters are learned.
- How 

### Advantages over MLP


### Multiple filters

- Can use multiple filters at the same time.
- One output vector for each filter; together forms a **output volume**.

### Multiple layers

- Use filters on the output of previous filters.
- Need to adjust the dimension accordingly.

### Start and end of data

- At the boundary
- Ignore the boundary
	- Leads to a reduction of dimension
	- Lost information
- Pad the boundaries
	- Add (filter length-1)/2 around the data to preserve the dimension
	- Add 0s

### How to reduce dimensions

- Take bigger steps. strides
- Use a pooling layer:
	- Goes over the data similar to a convolution
	- Applies a deterministic function like max or average on the input
	- Usually has stride == pool size.

## Higher Dimension Convolution

### 2D convolution


## Thesis topics

- offshore rocks identification
	- measurements from satelite, etc. to predict rock composition
- property recommendations for open street maps
- early recurrence of colorectal liver metastasis prediction from CY images and clinical data;