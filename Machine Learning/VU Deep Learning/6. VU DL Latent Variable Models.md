# Latent Variable Models


## Introduction to Generative Modeling

Generative models aim to capture the underlying distribution of data, enabling the generation of new, similar data points. 

Unlike discriminative models that focus solely on decision boundaries between classes, generative models learn the joint probability distribution of inputs and their labels, $P(X,Y)$. This comprehensive understanding allows them to generate realistic data samples and identify outliers effectively.

## Challenges in High-Dimensional Data Modeling

Modeling high-dimensional data, such as images with numerous pixels, presents significant challenges due to the complexity of dependencies among variables. Explicitly modeling these dependencies is often impractical. Latent variable models address this by introducing hidden variables that capture the underlying factors generating the observed data, simplifying the modeling process.

## Latent Variable Models: Definition

Latent variable models introduce hidden variables (Z) that are assumed to generate the observed data (X). The key assumption is that a few independent latent factors can explain the observed variability. For example, in facial images, latent factors might represent attributes like gender, eye color, or pose. The joint distribution P(X, Z) is defined, and learning both P(Z) and P(X|Z) facilitates effective modeling of P(X). A significant challenge is inferring the latent factors (Z) when only the input (X) is available.

- $P(Z)$: the distribution of latent variable Z
- P(X|Z): how X is generated from Z
- P(Z) defines the generative model’s latent space, from which new samples can be drawn.
- By sampling Z∼P(Z)Z \sim P(Z)Z∼P(Z) and passing it through P(X∣Z)P(X|Z)P(X∣Z), the model can generate new XXX samples.

## Autoencoders

Autoencoders are unsupervised learning architectures comprising two main component:

- Encoder: Maps input data from a high-dimensional space to a lower-dimensional latent space, learning the hidden factors (Z).
    
- Decoder: Reconstructs the input data from the latent representation, aiming to minimize reconstruction error.
    

In essence, autoencoders learn efficient codings of the input data, capturing essential features while reducing dimensionality. When linear mappings are used, autoencoders are closely related to Principal Component Analysis (PCA). However, introducing non-linearities allows autoencoders to capture more complex patterns in the data.

Problems with naive autoencoder:

|Aspect|Problem|Reason|
|---|---|---|
|Interpretability|Latent space is not interpretable or disentangled.|Objective focuses only on reconstruction error without disentangling factors of variation.|
|Separability|Latent space does not organize or separate classes meaningfully.|No explicit supervision or loss term encourages class or feature separation in ZZ.|
|Generative|Poor generative performance; random sampling from ZZ often results in meaningless outputs.|Latent space does not follow a well-defined distribution (e.g., Gaussian), and sampling is not trained.|

Advanced approaches like VAEs, GANs, and disentanglement regularization techniques address these limitations, making the models more interpretable, separable, and generative.

To generate, learn P(X|Z) and P(Z). Then sample from P(Z).

We want to maximize the log likelihood of P(X), which equals to integral of P(X|Z)P(Z) over all Zs

## Linear Latent Variable Models

Assuming linear relationships and Gaussian distributions, linear latent variable models can be analyzed tractably. 

For instance, Probabilistic Principal Component Analysis (PPCA) models the data as a linear transformation of latent variables with added Gaussian noise. The marginal distribution of the observed data remains Gaussian, characterized by specific mean and covariance structures. This framework allows for analytical solutions and efficient inference.

Since P(Z) is a gaussian, P(X|Z) is a linear transformation of P(Z), P(X|Z) is gaussian, and P(X) is also gaussian

P(Z|X) can be inferred from P(Z) and P(X|Z)

P(X|Z) acts as a decoder: given Z, what is X
P(Z|X) is an encoder: given X, what is Z

## Variational Inference for Latent Variable Models

In cases where the integral for the marginal likelihood is intractable, variational inference provides an efficient approximation method. The core idea is to approximate the true posterior distribution with a simpler, tractable distribution by maximizing the Evidence Lower Bound (ELBO). This approach balances reconstruction accuracy and regularization, facilitating efficient learning in complex models.

### Variational posterior Q

#### What is the Variational Posterior Q(Z|X)?

- The variational posterior Q(Z|X) is an **approximation** to the true posterior distribution P(Z∣X)P(Z|X), where:
    
    $P(Z|X) = \frac{P(X|Z)P(Z)}{P(X)}$
- **True Posterior Challenges**:
    
    - P(Z∣X)P(Z|X) is often **intractable** to compute because $P(X) = \int P(X|Z)P(Z)dZ$ involves a high-dimensional integral that cannot be evaluated analytically for most models.
- **Role of Q(Z∣X)Q(Z|X)**:
    
    - Q(Z|X) serves as a simpler, tractable distribution to approximate P(Z∣X)P(Z|X).
    - It is chosen from a family of parameterized distributions (e.g., Gaussian distributions with learnable mean and variance) and optimized to be as close as possible to P(Z∣X)P(Z|X).

#### How Is Q(Z|X) Optimized?

- The optimization goal is to make Q(Z∣X)Q(Z|X) approximate P(Z|X) well. This is done by minimizing the **Kullback-Leibler (KL) divergence** between Q(Z|X) and P(Z|X):
    
    $\text{KL}(Q(Z|X) \| P(Z|X)) = \mathbb{E}_{Q(Z|X)} \left[ \log Q(Z|X) - \log P(Z|X) \right]$
- Since P(Z|X) involves the intractable term P(X), variational inference instead maximizes the **Evidence Lower Bound (ELBO)**, which is equivalent to minimizing the KL divergence:
    
    $\text{ELBO} = \mathbb{E}_{Q(Z|X)}[\log P(X|Z)] - \text{KL}(Q(Z|X) \| P(Z))$

### ELBO

The **Evidence Lower Bound (ELBO)** is a fundamental concept in variational inference, used to approximate intractable likelihoods and perform efficient probabilistic learning in models like **Variational Autoencoders (VAEs)**.

#### What Does ELBO Stand For?

- ELBO stands for **Evidence Lower Bound**.
- It is a lower bound on the logarithm of the marginal likelihood log⁡P(X)\log P(X), where P(X)P(X) is the probability of the observed data under a generative model.

#### Why Is ELBO Needed?

The **marginal likelihood** P(X) is central to probabilistic modeling but often involves an intractable integral over latent variables Z:

$P(X) = \int P(X, Z) \, dZ = \int P(X|Z) P(Z) \, dZ$

- Computing P(X) exactly is usually infeasible due to the high-dimensional integration.
- ELBO provides a tractable objective that approximates $\log P(X)$ and allows for efficient optimization of the model.

#### Mathematical Definition of ELBO

The **ELBO** is derived from the marginal likelihood P(X)P(X) and is expressed as:

$\log P(X) \geq \text{ELBO} = \mathbb{E}_{Q(Z|X)}[\log P(X|Z)] - \text{KL}(Q(Z|X) \| P(Z))$

 **Key Terms**:

1. **Reconstruction Term**:
    
    $\mathbb{E}_{Q(Z|X)}[\log P(X|Z)]$
    - Measures how well the model reconstructs the observed data X from the latent variables ZZ.
    - Encourages $P(X|Z)$ (the decoder in a VAE) to generate realistic data given Z.
    - Taking the expectation over $Q(Z|X)$ means that we average $\log P_\theta(X|Z)$ over the range of plausible $Z$ values weighted by their probability under $Q(Z|X)$.
    - P(Z) is the prior distribution of Z, usually assumed to be gaussian for simplicity in training the model

2. **KL Divergence Term**:
    
    $\text{KL}(Q(Z|X) \| P(Z))$
    - Measures how close the variational posterior Q(Z|X) is to the prior P(Z).
    - Acts as a regularization term, ensuring that the learned latent space Q(Z|X) aligns with the prior distribution P(Z).

#### Rearranged Form of ELBO:

The marginal likelihood P(X)P(X) can be rewritten as:

$\log P(X) = \text{ELBO} + \text{KL}(Q(Z|X) \| P(Z|X))$

Since KL divergence is always non-negative, the ELBO is a lower bound:

log⁡P(X)≥ELBO\log P(X) \geq \text{ELBO}


#### Intuition Behind ELBO

- **Decomposition**:
    
    - ELBO combines two objectives:
        1. **Reconstruction**: Ensuring the model can accurately reconstruct data XX from latent variables ZZ.
        2. **Regularization**: Encouraging the posterior Q(Z|X) to resemble the prior P(Z), which makes the latent space well-organized and smooth for sampling.
- **Optimization Goal**:
    
    - Maximizing ELBO corresponds to maximizing the likelihood of the data P(X) while ensuring a balance between accurate reconstruction and regularized latent variables.

### How close is ELBO and the real P(X)

The gap between our lower bound and the real $p_\theta(X)$ is determined by how closely our approximation $Q(Z|X)$ aligns with the true posterior distributions of $p_\theta(Z|X)$

$D_{KL}(q_\phi(z|x));p_\theta(z|x))$

When Q is chosen well, maximizing ELMO is synonymous to maximizing P(X).

Closing the gap between Q and P(Z|X) is synergistic with maximizing ELBO:

- **Maximizing ELBO**:
    
    - When we maximize the ELBO, we improve the model's ability to explain the data by optimizing the reconstruction term EQ(Z∣X)[log⁡P(X∣Z)]\mathbb{E}_{Q(Z|X)}[\log P(X|Z)]EQ(Z∣X)​[logP(X∣Z)] and regularizing the latent space via −KL(Q(Z∣X)∥P(Z))-\text{KL}(Q(Z|X) \| P(Z))−KL(Q(Z∣X)∥P(Z)).
    - This inherently reduces the KL divergence KL(Q(Z∣X)∥P(Z∣X))\text{KL}(Q(Z|X) \| P(Z|X))KL(Q(Z∣X)∥P(Z∣X)) because a better approximation Q(Z∣X)Q(Z|X)Q(Z∣X) leads to a tighter bound.
- **Closing the Gap**:
    
    - By improving Q(Z∣X)Q(Z|X)Q(Z∣X) such that it closely approximates P(Z∣X)P(Z|X)P(Z∣X), we reduce KL(Q(Z∣X)∥P(Z∣X))\text{KL}(Q(Z|X) \| P(Z|X))KL(Q(Z∣X)∥P(Z∣X)).
    - When KL(Q(Z∣X)∥P(Z∣X))=0\text{KL}(Q(Z|X) \| P(Z|X)) = 0KL(Q(Z∣X)∥P(Z∣X))=0, the ELBO becomes equal to the true log marginal likelihood: log⁡P(X)=ELBO.\log P(X) = \text{ELBO}.logP(X)=ELBO.

