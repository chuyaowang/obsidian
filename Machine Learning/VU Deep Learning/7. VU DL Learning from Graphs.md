# Learning from graphs


## Introduction to Learning with Graphs

Graphs are fundamental structures that model relationships between entities. They are prevalent in various real-world applications, including social networks, knowledge representation, and recommendation systems. Understanding how to learn from graphs is crucial for tasks such as node classification, link prediction, and clustering.

### Undirected graph

- simple graph
	- connected parts and unconnected parts
	- the graph is connected when all nodes are connected
- self loop
	- a node connects to itself
- multigraph
	- multiple edges between nodes

### Directed graph

- Edges have directions
- Directed and undirected edges can co-exist in a graph

### Labels and types

Edges and nodes can be labeled.
Node can be divided into types.

### Resource description framework

Each node and edge labeled with URL or literal value. Makes the nodes and edges unique.

### Information on edges

Label edge with weights: interpretation depends on how you analyze the graph.
Properties labeled on edges: the additional details are known as qualifiers

## Embeddings

Embeddings: low-dimensional representations of objects. Low dimension, in the context of embeddings, means a representation much smaller than the original size.

### Image embeddings

Some embedded spaces exhibit a valuable structure that allows navigation. Moving in specific directions within this space holds meaningful changes in certain features of the object. 

For example, in images, navigating could transition from a smiling person to a non-smiling one or from a female to a male gender along a certain direction.

### Word embeddings

**Distributional hypothesis**: understanding a word involves its contextual associations.

For instance, words like 'cat,' 'dog,' and 'fish' frequently co-occur with 'animal' but rarely with 'loudspeaker.' This implies that 'cat,' 'dog,' and 'fish' are semantically similar, while 'loudspeaker' is distinct. Leveraging such contextual information allows us to create embeddings for words.

## Why graphs in ML?

The primary goals include classification, regression, or clustering of nodes, edges, or the entire graph. 

For instance, in an Amazon product graph with missing labels, we might want to _classify_ nodes, such as determining whether a product is poisonous or not. 

Regression could _quantify_ toxicity levels. 

Other applications include recommender systems for _product suggestions_ based on connections in the knowledge graph, document modeling to _find similar documents_ through graph links, and aligning graphs to identify _similarities between nodes_. 

Additionally, tasks like _link prediction_ and _error detection_ aim to refine and correct the knowledge graph by predicting missing links and rectifying inaccuracies.

## Graph Embedding Techniques

Graphs can't be neatly represented in a linear fashion. Therefore, feeding them into our network becomes a non-trivial task.

Traditional ML on graphs: manual feature engineering, suffers from scalability issues and task specific

### Node embeddings

Instead, **embedding knowledge graphs in a vector space**. Each node is transformed to a vector in an effective space.

What we're essentially doing is akin to something called **propositionalization**. This involves creating one vector for each entity or node in the graph. 

The primary goal of this embedding is to preserve information from the original graph. Ideally, we prefer an unsupervised approach for now, meaning the embeddings obtained are task and dataset independent. They originate from a specific graph but should be universally applicable.

Efficacy is crucial. Some networks can have millions of nodes and connections. The embedding should be low dimensional and compact to save space.

### Two visions on node embedding

- Preserving topology: neighbors in the graph are neighbors in the embedding
- Preserving similarity: similar nodes have similar embeddings

### Two major targets

- Improving the original data: related to the graph itself
	- knowledge graph completion
	- link prediction
	- anomaly detection
- For downstream ML tasks:
	- Classification, regression, clustering, KNN
	- part of a larger process
		- QA/dialog system
		- Translation
		- Image segmentation: if entity is a cow, background is more likely to be farm rather than sea

## Three approaches of graph embedding

- **Translational**: any relation existing in the knowledge graph is directly mapped to its counterpart in the embedded space
- **Tensor factorization**:  involves creating a 3D matrix of all the relations in the graph. Then factorize it. Reconstructing the original hints which edges were _missing_
- **Random walk based**: the process includes starting at a particular node and navigating its _context_ for embedding. This method incorporates the distributional hypothesis.

### TransE

- Translational embedding
- In large knowledge graphs
	- Nodes
	- Typed edges
- Generate embedding for nodes connected by typed edge. Then generate embeddings for the typed edges so they serve as translation from one node to the other one that the edge is connected to.
- When an edge is used repeatedly, the same edge embedding is used.
- Optimization goal: align the head ('h') of the relationship, the edge, and the tail
	- head+relation should be as close to tail as possible
	- it should also penalize instances where incorrect triples or relations in the graph are in close proximity
		- a triple: head, translation, tail
		- valid triple: (Paris, IsCapitalOf, France)
		- invalid triple: (Paris, IsCapitalOf, America)
- Done by taking head or tail of the triple and swapping it for something else random to create false triples.
- Loss function: $$\mathcal{L} = \sum_{(h, r, t) \in \mathcal{T}} \max(0, \gamma + \|\mathbf{h} + \mathbf{r} - \mathbf{t}\| - \|\mathbf{h}' + \mathbf{r} - \mathbf{t}'\|)$$
    - Explanation of Terms:
        - $\mathcal{T}$: Set of all training triples.
        - $\mathbf{h}, \mathbf{r}, \mathbf{t}$: Embeddings of the head, relation, and tail for a valid triple.
        - $\mathbf{h}', \mathbf{r}, \mathbf{t}'$: Embeddings for an invalid triple, often created by corrupting the head or tail.
        - $\gamma$: Margin that separates valid and invalid triples.
    - How It Works:
        - The loss enforces:
            - Valid triples ($\mathbf{h}, \mathbf{r}, \mathbf{t}$) to have small distance $\|\mathbf{h} + \mathbf{r} - \mathbf{t}\|$.
            - Invalid triples ($\mathbf{h}', \mathbf{r}, \mathbf{t}'$) to have large distance $\|\mathbf{h}' + \mathbf{r} - \mathbf{t}'\|$.
        - The loss penalizes the model when: $\|\mathbf{h} + \mathbf{r} - \mathbf{t}\| + \gamma > \|\mathbf{h}' + \mathbf{r} - \mathbf{t}'\|$
        - This means:
            - If the distance for the valid triple (∥h+r−t∥\|\mathbf{h} + \mathbf{r} - \mathbf{t}\|∥h+r−t∥) is **not sufficiently smaller** than the distance for the invalid triple (∥h′+r−t′∥\|\mathbf{h}' + \mathbf{r} - \mathbf{t}'\|∥h′+r−t′∥), the loss becomes positive.
            - The model is then updated to decrease the valid triple distance and/or increase the invalid triple distance to satisfy the margin constraint.

- Advantage: conceptually easy
- Disadvantage: 
	- poor embedding quality
	- the better the model, the less scalable
	- one hop
### Tensor factorization

- Tensor example:
	- x: entities
	- k: relations
	- y: whether relation exists
- Factorize the tensor to obtain the embeddings
- The embedding discards noise, anomalies, and mistakes in the graph, but also generalizes, predicting missing edges not in the original graph.

- Advantage:
	- Conceptually easy
	- Good for link prediction
	- Usually scalable
- Neutral:
	- Explainable
	- Numeric attributes can be included somehow
- Disadvantage:
	- Embedding quality for ML task
	- The better the model, the less scalable

### Random walk based

- Randomly walk around a node, generate a sequence of nodes and their labels
- Feed the sequence to word2vec to generate embeddings
	- Example sequence: This person likes milk. Milk is similar to cheese, and cheese contains a high amount of fat.
- Weighting can be used to bias some directions during the walk
- Graph kernels can be used to navigate the graph

Personalized PageRank:
- Create co-occurrence stats using Personalized PageRank
	- All pairs' PPR
- Apply the GloVe model
	- Similar things will have similar context
	- Optimizes for preserving analogy
		- King - man + woman = queen
		- Berlin - Germany + Austria = Vienna
- Complete context captured compared to random walk

- Advantage:
	- deals with large graphs
	- good embeddings
	- good training time
- Neutral:
	- likely or partially explainable
	- larger context used
- Disadvantage: 
	- link prediction: these methods prioritize semantic similarity based on the distributional hypothesis, potentially overlooking the original graph's structure

- Some examples:
	- **Node2Vec**: Employs biased random walks to capture network neighborhoods, generating node embeddings that reflect community structures.
	- **DeepWalk**: Utilizes truncated random walks to learn latent representations by treating walks as sentences in a language model.
	- **LINE**: Preserves both first-order (local) and second-order (global) proximities in the network, ensuring embeddings capture both immediate and broader network structures.

## Graph neural networks

- Creating embeddings first then use it for ML tasks is not end-to-end learning
	- certain information deemed unimportant during the embedding process might be discarded, potentially impacting the application.
- With end-to-end learning, a different challenge arises. The embeddings generated in such a system may lack generalizability, being too specific to the task at hand. Unlike the two-step approach, they may not be transferable to other tasks.

### Graph convolutional neural network

- We can convert a graph to a multilayer network, with each layer repeating the connections in the original network
- Also add self-loops in the network, represented as connection between a node to itself in the next layer
- Each connection is a small MLP. The input is a node, which is a n-dimensional vector, the output is also a vector. 
- The weight is shared for all connections in this layer and can be learned.
	- resembles how filters are shared in CNN, hence the name GCNN
- In practice, it scales not exceptionally well to large graphs
	- with clever engineering, scalability can be improved. ex. python Geometric library
- Another issue is the need for increased normalization:
	- some nodes are more connected; when information is aggregated through layers, these nodes have higher feature values
	- this causes learning to bias towards the more connected nodes
	- normalization resolves this issue

Compact representation of GCN:

$H^{(l+1)} = \sigma(\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)})$

- $H^{(l)}$: Node features at layer $l$.
- $\tilde{A}$ = A + I: Adjacency matrix with added self-loops.
- $\tilde{D}$: Degree matrix of $\tilde{A}$, where $\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$.
- $\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$: Normalized adjacency matrix.
- $W^{(l)}$: Trainable weight matrix.
- $\sigma$: Activation function.

### Example of tasks

- node classification
	- a node's type
- regression of attributes
	- predict the price of a product based on its connection with other nodes
- regression/classification on the complete graph
	- predict boiling point of a molecule: represent the molecule as a graph, pass it through the network, collect outputs, and use them for regression
	- predict toxic/not toxic for a molecule: same process, collect information, and make a prediction

### RGCN
#### Type edges

- one weight matrix for each edge type
- reversed edge:
	- sometimes the default direction does not align with application
	- reverse edge: 'I live in Amsterdam' and 'Amsterdam has inhabited me'

#### Formulation

The core idea is to extend the GCN aggregation mechanism to account for multiple edge types (relations). The node embeddings at layer $l+1$ are computed as:

$$\mathbf{h}_v^{(l+1)} = \sigma \left( \sum_{r \in R} \sum_{u \in \mathcal{N}_r(v)} \frac{1}{c_{v, r}} W_r^{(l)} \mathbf{h}_u^{(l)} + W_0^{(l)} \mathbf{h}_v^{(l)} \right)$$
- Explanation of Terms:

1. $\mathcal{N}_r(v)**$:
    
    - The set of neighbors of node v that are connected via relation r.
2. **Relation-Specific Transformations ($W_r^{(l)}$)**:
    
    - Each relation $r \in R$ has its own weight matrix $W_r^{(l)}$ at layer $l$.
    - This models the different effects that different relations have on message propagation.
3. **Self-Loop Transform ($W_0^{(l)}$)**:
    
    - A separate weight matrix $W_0^{(l)}$ is used for the self-loop connection, allowing a node's own features to influence its updated embedding.
4. **Normalization Factor ($c_{v, r}$)**:
    
    - A normalization constant (e.g., based on the degree of vv under relation rr) to balance contributions from neighbors.
5. **Activation Function ($\sigma$)**:
    
    - A non-linear activation function, such as ReLU.

---

#### R-GCN Summary of Operations

For each node v in the graph:

1. **Message Passing**:
    - Aggregate messages from neighbors for each relation rr using Wr(l)W_r^{(l)}.
2. **Self-Loop Update**:
    - Add the transformed self-loop contribution W0(l)hv(l)W_0^{(l)} \mathbf{h}_v^{(l)}.
3. **Normalization**:
    - Normalize the aggregated messages to prevent exploding or vanishing values.
4. **Non-Linearity**:
    - Apply an activation function to compute the final embedding.

#### Key Characteristics of R-GCN

- **Relation-Specific Weight Matrices**:
    - Enables the model to learn distinct transformations for each relation type.
- **Self-Loop Connections**:
    - Ensures that nodes retain their own information at each layer.
- **Parameter Efficiency**:
    - R-GCN uses techniques like basis decomposition or block diagonal matrices to reduce the number of parameters when the number of relations (∣R∣|R|) is large.

#### Simplified Version for 1 Relation

If there is only one relation (or no differentiation between relations), the R-GCN simplifies to a standard GCN:

$$\mathbf{h}_v^{(l+1)} = \sigma \left( \sum_{u \in \mathcal{N}(v)} \frac{1}{c_{v}} W^{(l)} \mathbf{h}_u^{(l)} + W_0^{(l)} \mathbf{h}_v^{(l)} \right)$$

This shows that R-GCN generalizes GCNs to handle multi-relational data effectively.

#### Applications of R-GCN

- **Knowledge Graph Completion**:
    - Predict missing edges or node attributes in knowledge graphs with multi-relational edges.
- **Social Network Analysis**:
    - Capture different types of relationships in social networks (e.g., friendships, professional ties).
- **Recommender Systems**:
    - Handle diverse user-item interactions with relation-specific dynamics.

#### Challenges and Solutions

- **Parameter Explosion**:
    - When the number of relations |R| is large, the number of parameters grows linearly with |R|.
    - **Solution**: Use parameter sharing techniques like basis decomposition or block diagonalization.
- **Scalability**:
    - Large graphs with many nodes and edges can make message passing computationally expensive.
    - **Solution**: Employ sampling methods (e.g., GraphSAGE) to reduce computational overhead.

### Flexibility of GCN

the flexibility in graph neural networks allows for experimentation with various network architectures. Instead of sticking to Multi-Layer Perceptrons (MLPs) for each edge, you can explore different network types. For instance, you could replace an MLP with an LSTM or any other type of recurrent network architecture. Taking it further, you could even introduce convolutional neural networks (CNNs) into the mix. This would mean that one node's input could be an image, and the transformation during the edge operation would involve processing that image through the CNN.

The possibilities are wide open, and researchers often try out different approaches to see what works best for a given scenario. Additionally, the notion of shared weight matrices can also be explored in various ways. Instead of having shared weights all the way, you might experiment with multiple weight matrices, akin to having multiple filters in convolutional layers. The sharing of weights can extend not only between edges but also deeper into the network, allowing for reuse of weights at different levels. Advanced techniques, such as tying weights in specific ways, are also possible.

## Application: Query Embedding

Query embedding involves encoding complex queries over knowledge graphs into a vector space, facilitating efficient query processing and reasoning. This approach enables models to handle intricate query patterns beyond simple link prediction. Techniques in this domain often leverage GNNs to capture the structural and relational complexities inherent in queries.

### Mathematical Foundations

#### Graph Representation

A graph GG is defined as G=(V,E)G = (V, E), where VV is the set of vertices (nodes) and EE is the set of edges connecting the vertices. Each edge e∈Ee \in E can be represented as a tuple (u,v)(u, v), indicating a connection between nodes uu and vv.

#### Adjacency Matrix

The structure of a graph can be encoded in an adjacency matrix AA, where Aij=1A_{ij} = 1 if there is an edge from node ii to node jj, and Aij=0A_{ij} = 0 otherwise. For weighted graphs, AijA_{ij} holds the weight of the edge between nodes ii and jj.

#### Graph Laplacian

The graph Laplacian LL is defined as:

L=D−AL = D - A

where DD is the degree matrix, a diagonal matrix where DiiD_{ii} represents the degree of node ii. The Laplacian is instrumental in spectral graph theory and plays a significant role in GCNs.

#### Graph Convolutional Networks (GCNs)

GCNs perform convolution operations on graphs, defined as:

H(l+1)=σ(D~−1/2A~D~−1/2H(l)W(l))H^{(l+1)} = \sigma\left( \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)} \right)

where:

- A~=A+I\tilde{A} = A + I is the adjacency matrix with added self-loops.
    
- D~\tilde{D} is the degree matrix of A~\tilde{A}.
    
- H(l)H^{(l)} is the matrix of activations in the ll-th layer; H(0)H^{(0)} is the input feature matrix.
    
- W(l)W^{(l)} is the trainable weight matrix for the ll-th layer.
    
- σ\sigma is an activation function, such as ReLU.
    

This operation aggregates feature information from neighboring nodes, enabling the network to learn representations that capture local graph structures.

#### Node Embeddings

The objective of node embedding techniques is to map nodes to a dd-dimensional vector space (d≪∣V∣d \ll |V|) such that the graph's structural information and properties are preserved. Formally, this can be represented as a function f:V→Rdf: V \rightarrow \mathbb{R}^d.

#### Loss Functions in Graph Learning

In supervised graph learning tasks, models are trained to minimize a loss function that measures the discrepancy between the predicted outputs and the actual labels. For node classification, a common loss function is the cross-entropy loss:

L=−∑i∈YL∑c=1CYiclog⁡(Y^ic)\mathcal{L} = -\sum_{i \in \mathcal{Y}_L} \sum_{c=1}^C Y_{ic} \log(\hat{Y}_{ic})

where:

- YL\mathcal{Y}_L is the set of labeled nodes.
    
- CC is the number of classes.
    
- YicY_{ic} is a binary indicator (0 or 1) if node ii belongs to class cc.
    
- Y^ic\hat{Y}_{ic} is the predicted probability of node ii belonging to class cc.
    

Minimizing this loss encourages the model to produce accurate class probability distributions for the nodes.

## Conclusion

Learning from graphs is a dynamic and impactful area within deep learning, offering tools and methodologies to effectively model and analyze relational data.