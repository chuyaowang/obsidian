# Pairwise Sequence Alignment

[0. VU Algorithms in Sequence Analysis](0.%20VU%20Algorithms%20in%20Sequence%20Analysis.md)
[2. MIT CompBio - Dynamic Programming](2.%20MIT%20CompBio%20-%20Dynamic%20Programming.md)
[2. VU FoB - Mutation, Evolution, and Sequence Alignment](2.%20VU%20FoB%20-%20Mutation,%20Evolution,%20and%20Sequence%20Alignment.md)

## Sequence Alignment

- The process of comparing two or more strings letters (nucleotides or amino acids) to:
	- infer their similarity
	- visualize their similarity
- Pairwise sequence alignment: comparing two strings
- Useful for: genome assembly, phylogeny reconstruction

### Online sequence alignment

- The sequences cannot be pre-processed to build an index on them
- Four main approaches to online sequence alignment:
	- dynamic programming (DP) based algorithms (this course); 
	- automata-based algorithms; 
	- bit-parallel algorithms; 
	- filtering-based algorithms
- Distance:
	- Edit distance
	- Hamming distance
- Biological applications
	- Global alignment
	- Local alignment

## Definitions

- Alphabet: an _alphabet_ $\Sigma$ is a set whose elements are called letters.
- String: a _string_ on $\Sigma$ is a sequence of elements from $\Sigma$.
	- Empty String: a zero letter sequence is called an _empty string_ $\varepsilon$. 
	- All possible strings: the set of _all possible strings_ on the alphabet $\Sigma$ is denoted by $\Sigma^*$ .
- Length of string: the _length of a string_ $x$ is the length $|x|$ of the sequence.
- Identity between strings: $x=y$ iff $|x|=|y|$ and $x[i]=y[i]$ for all $0\le i\le |x|$.
- Concatenation: the _concatenation_ of $x$ and $y$ is the string of the letters of $x$ followed by letters of $y$, denoted by $xy$.
- Substring of string: a string $x$ is a _substring_ of a string $y$ if there exist two strings $u$ and $v$ such that $y=uxv$. $u$ and $v$ can be empty strings.
- Occurrence of string: let $x$ be a non-empty string and $y$ be a string. We say that there exists an _occurrence_ of $x$ in $y$, or, more simply, that $x$ occurs in $y$, when x is a substring of y.

### Distances

- Distance between two strings: we say that a function $\delta$: $\Sigma^*\times \Sigma^*\rightarrow \mathbb{R}$ is a _distance_ on $\Sigma^*$ if the following properties are satisfied: for every $u,v\in \Sigma^*$,
	- Positivity: $\delta(u,v)\ge 0$;
	- Separation: $\delta(u,v)=0$ iff $u=v$;
	- Symmetry: $\delta(u,v)=\delta(v,u)$;
	- Triangle inequality: $\delta(u,v)<\delta(u,w)+\delta(w,v)$, for $w\in\Sigma^*$.
- The distances are defined from operations that transform $x$ to $y$:
	- _Substitution_ (sub) of a letter of x by a letter of y;
	- _Insertion_ (ins) of a letter of y in x;
	- _Deletion_ (del) of a letter of x.
- Edit distance: $\delta_E(x,y)=\operatorname{min}\{cost\ of\ \sigma:\sigma \in S_{x,y}\}$, where $\sigma$ is a sequence of elementary edit operations that transform $x$ into $y$. $S_{x,y}$ is the set of $\sigma$s, and the cost of an element $\sigma \in S_{x,y}$ is the sum of the costs of the edit operations of the sequence $\sigma$. The function $\delta_E$ is then a distance on $\Sigma^*$ called the _edit distance_.
- Hamming distance: the _Hamming distance_ $\delta_H(x,y)$ is deﬁned for two strings $x$ and $y$ of the same length to be the number of positions in which $x$ and $y$ possess different letters. Hence, the Hamming distance only accounts for substitutions.

> [!note] Interpreting edit distance
> We always choose the sequence of operations with the lowest editing cost out of all possible sequences.

^aaddb5

### Alignment between two strings

An alignment between $x$ and $y$ is a string $z$ on the alphabet of pairs of letters, more accurately on $$(\Sigma\cup\{\varepsilon\})\times(\Sigma\cup\{\varepsilon\})\setminus(\{\varepsilon,\varepsilon\})$$ whose ($z$'s) projection on the first component is x, and the projection on the second component is y. Thus, if z is an alignment of length p between x and y, we have $$\begin{aligned}z&=(x_0^{\prime},y_0^{\prime})(x_1^{\prime},y_1^{\prime})\ldots(x_{p-1}^{\prime},y_{p-1}^{\prime})\\x&=x_0^{\prime}x_1^{\prime}\ldots x_{p-1}^{\prime}\\y&=y_0^{\prime}y_1^{\prime}\ldots y_{p-1}^{\prime}\end{aligned}$$
- $\Sigma\cup\{\varepsilon\}$ is the alphabet of symbols $x$ and $y$ are built from plus a gap represented by $\varepsilon$;
- $\times$ represents the [Cartesian Product](Cartesian%20Product.md). $\Sigma\cup\{\varepsilon\}\times\Sigma\cup\{\varepsilon\}$ gives all the combinations of characters in the alphabet;
- $\setminus$: set difference symbol; 
-  $\setminus(\{\varepsilon,\varepsilon\})$ means excluding the pair that is both gaps.
- projection: the operation of extracting one specific component from each pair in a sequence of aligned pairs, which is $z$. Therefore, if we take all the first components of each pair in $z$, we get $x$ with necessary gaps back, same with y.

## Alignment algorithms

### Alignment with dynamic programming

[Global Alignment (Review)](3.%20MIT%20CompBio%20-%20Hashing,%20Database%20Search,%20BLAST%20algorithm.md#Global%20Alignment%20(Review))

Edit distance matrix: opposite of [alignment score matrix](2.%20MIT%20CompBio%20-%20Dynamic%20Programming.md#Traversing%20the%20matrix). Add the edit distances and choose the minimum.

Define $\text{sub}(a,b)\coloneqq 3, \text{ins}(a)=\text{del}(a)\coloneqq 1, \text{sub}(a,a)=0$
If there is no editing (insertion, deletion, substitution), the editing distance does not change

**Initialization**: $F(0,0)=0$
- Top left

**Iteration**: $$F(i,j)=\min\quad\begin{cases}&F(i-1,j)+\text{del}(x[i])\\&F(i,j-1)+\text{ins}(y[j])\\&F(i-1,j-1)+\text{sub}(x[i],y[j])\end{cases}$$

**Termination**: Bottom right

> [!note] Finding the optimal alignment
> There could be exponentially (in n) many sequence of edits, even optimal ones with the same [edit distance](#^aaddb5). With the DP-based algorithm, we can compute at least one of those optimal ones.
> 
> If we follow every possible path from the last cell (bottom right) to the first cell (top left) then we will find all of the optimal alignments. The problem is that in practice this will cost a lot and so biologists settle for a few optimal alignments and then pick the one that makes more sense to them depending on other biological knowledge.

### Traceback

[Backtrack](2.%20MIT%20CompBio%20-%20Dynamic%20Programming.md#Backtrack)

The pointers: each cell in row zero points to the cell to its left, and each cell in column zero points to the cell just above it. Other pointers are based on where the minimal editing cost comes from.

**High road traceback**: when there are multiple paths to trace back, take the path that is the highest on the pointers matrix.
**Low road traceback**: take the lowest path.
### Complexity

Time and space complexity: $mn$ for S1 with $m$ letters and S2 with $n$ letters.

## Needleman-Wnusch Algorithm

[Dynamic Programming for Sequence Alignment](2.%20MIT%20CompBio%20-%20Dynamic%20Programming.md#Dynamic%20Programming%20for%20Sequence%20Alignment)

Instead of using editing distance, the Needleman-Wnusch algorithm measures sequence similarity, but the principle is the same.

**Initialization**: $F(0,0)=0$
- Top left

**Iteration**: $$F(i,j)=\max\quad\begin{cases}&F(i-1,j)-d\\&F(i,j-1)-d\\&F(i-1,j-1)+s(x_i,y_j)\end{cases}$$, where $d$ is gap penalty, and $s$ can be positive (reward) or negative (penalty) depending on if $x_i$ and $y_j$ are the same or different.

**Termination**: Bottom right

## Smith-Waterman Algorithm

[Local alignment](3.%20MIT%20CompBio%20-%20Hashing,%20Database%20Search,%20BLAST%20algorithm.md#Local%20Alignment)

The local alignment algorithm.

**Initialization**: $F(0,j)=F(i,0)=0$. 
- The first column and top row are all 0s
- This allows initialization from anywhere. In contrast to global alignment where $F(0,0)=0$, allowing initialization from the top left only.

**Iteration**: $$F(i,j)=\max\quad\begin{cases}&0\\&F(i-1,j)-d\\&F(i,j-1)-d\\&F(i-1,j-1)+s(x_i,y_j)\end{cases}$$Having 0 allows restarting anywhere.

**Termination**: Anywhere

### Traceback

- Locate one among the equally largest values in table S. 
- Traceback the path from the cell of this value by following the arrows, similarly as with the edit-distance algorithm.
- Stop the scan on a zero value.

### Complexity

$O(mn)$ for time and space.

### Waterman-Eggert algorithm

What if we want all local alignments of score $\ge$ t?
- Retrieve the current highest scoring alignment.
- Set its trace to 0 and recalculate the affected cell.
- Find the maximum again and traceback.

## Improvements

[Improvements on the algorithm](2.%20MIT%20CompBio%20-%20Dynamic%20Programming.md#Improvements%20on%20the%20algorithm)

### Bounded dynamic programming

[Bounded Dynamic Programming](2.%20MIT%20CompBio%20-%20Dynamic%20Programming.md#Bounded%20Dynamic%20Programming)

If no more than $d$ indels (insertion/deletions) are allowed, it reduces the area of the matrix that needs to be computed.

The running time is reduced from $O(mn)$ to $O(d\times \min\{m,n\})$

### Generalized gap penalty

[A general gap penalty function F(gap_length)](3.%20MIT%20CompBio%20-%20Hashing,%20Database%20Search,%20BLAST%20algorithm.md#A%20general%20gap%20penalty%20function%20F(gap_length)): not efficient.

### Affine gap penalty

[The paper on affine gap penalty](https://web.njit.edu/~usman/courses/bnfo601_fall18/affinetraceback.pdf)
[Related slides](https://www.cs.unc.edu/~anwica/comp555/Lecture15.pdf)

[Affine gap penalty](3.%20MIT%20CompBio%20-%20Hashing,%20Database%20Search,%20BLAST%20algorithm.md#Affine%20gap%20penalty): initial gap cost and continuing gap cost.
Using 3 matrices D, I, and M
## Semi-global alignment

[Semi-global alignment](3.%20MIT%20CompBio%20-%20Hashing,%20Database%20Search,%20BLAST%20algorithm.md#Semi-global%20alignment)

- Finding a gene in a genome
- Placing marker onto a chromosome
- One sequence much longer than the other

