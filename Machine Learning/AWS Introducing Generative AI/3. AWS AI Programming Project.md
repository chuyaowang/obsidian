# AI programming project

[0. AWS Introducing Generative AI](Machine%20Learning/AWS%20Introducing%20Generative%20AI/0.%20AWS%20Introducing%20Generative%20AI.md)

## Amazon SageMaker
 
Here are the detailed steps to set up Amazon SageMaker and create a Jupyter Notebook:
### Step 1: Sign in to AWS Management Console
1. Go to the [AWS Management Console](https://aws.amazon.com/console/).
2. Sign in with your AWS account credentials.

### Step 2: Navigate to Amazon SageMaker
1. In the AWS Management Console, search for "SageMaker" in the services search bar.
2. Click on "Amazon SageMaker" to open the SageMaker dashboard.

### Step 3: Create a New Notebook Instance
1. In the SageMaker dashboard, click on "Notebook instances" in the left sidebar.
2. Click the "Create notebook instance" button.

### Step 4: Configure the Notebook Instance
1. **Notebook instance name**: Enter a name for your notebook instance.
2. **Instance type**: Choose an instance type (e.g., `ml.t2.medium` for a general-purpose instance).
3. **IAM role**: Create a new role or choose an existing IAM role that has permissions to access S3 buckets and other necessary resources. If creating a new role, select "Create a new role" and choose "Any S3 bucket" for access.

### Step 5: Configure Additional Settings (Optional)
1. **Git repositories**: If you want to associate a Git repository, you can specify it here.
2. **Lifecycle configuration**: You can set up lifecycle configurations for your notebook instance if needed.

### Step 6: Create the Notebook Instance
1. Click the "Create notebook instance" button at the bottom of the page.
2. Wait for the status of your notebook instance to change from "Pending" to "InService." This may take a few minutes.

### Step 7: Open Jupyter Notebook
1. Once the notebook instance is in service, click on the "Open Jupyter" link next to your notebook instance.
2. This will open the Jupyter Notebook interface in a new tab.

### Step 8: Create a New Notebook
1. In the Jupyter Notebook interface, click on the "New" button on the right side.
2. Select "Python 3" (or the appropriate kernel) to create a new notebook.

### Step 9: Start Coding
1. You can now start writing your code in the new notebook. For example, you can implement the chatbot logic as discussed in the video.

### Step 10: Save Your Work
1. Remember to save your notebook regularly by clicking on the disk icon or using `File > Save and Checkpoint`.

If you have any specific questions about any of these steps or need further assistance with coding in the notebook, feel free to ask!

> Complete a 3-hour course in AWS Educate, do a project, and complete the final evaluations

## Course: ML introduction

- Careers:
	- Data scientist
	- ML engineer
	- Applied scientist
	- ML developer

## Course: AI and ML

- AI > ML > DL

### Terms

- **Model**: A model in machine learning is the trained algorithm that you use to identify patterns in your data. A model is trained through the machine learning process and captures and stores patterns. The more patterns a model stores, the bigger it will become. Machine learning model sizes are typically described in megabytes. The purpose of a model is to predict outcomes to new data inputs, based on previously observed patterns.
- **Training-algorithm**: In machine learning, you can use training algorithms to help you accomplish empirical tasks that require some sort of inductive inference. This objective involves induction because it uses data to train algorithms to make generalizable inferences. Thus, algorithms can make statistically reliable predictions or decisions, or complete other tasks, when they apply new data that was not used to train them. An algorithm-trained algorithm identifies patterns in your data without requiring any explicit, manually set rules.
- **Computer vision**: With computer vision, machines can identify people, places, and things in images with accuracy at or above human levels with much greater speed and efficiency. Often built with deep learning models, it automates extraction, analysis, classification, and understanding of useful information from a single image or sequence of images. The image data can take many forms, such as single images, video sequences, views from multiple cameras, or three-dimensional data.
- **Artificial neural network**: An artificial neural network is a collection of connected units or nodes that are used to build an information model based on biological systems. Each node is called an artificial neuron and mimics a biological neuron. It receives an input (stimulus), becomes activated if the input signal is strong enough (activation), and produces an output predicated on the input and activation. It's widely used in machine learning because an artificial neural network can serve as a general-purpose approximation to any function.
- **Compute model instances**: AWS provides a variety of instance types that support machine learning models, depending on the use case of the model. Amazon Elastic Compute Cloud (Amazon [EC2 Instance Types](AWS/Cloud%20Practitioner/2.2%20Compute%20in%20the%20Cloud.md#EC2%20Instance%20Types)) provides a wide selection of instance types (or families) that are optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity. They give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes so that you can scale your resources to the requirements of your target workload. AWS ML services, like Amazon SageMaker, use these instances to back computing needs for your ML projects.
- **Inference**: Inference is the process of making predictions using a trained model. In deep learning applications, inference accounts for up to 90 percent of total operational costs for two reasons. First, standalone graphics processing unit (GPU) instances are typically designed for model training—not for inference. Though training jobs batch process hundreds of data samples in parallel, inference jobs usually process a single input in real time. Thus, they consume a small amount of GPU compute, which makes standalone GPU inference cost-inefficient. On the other hand, standalone CPU instances are not specialized for matrix operations, and thus are often too slow for deep learning inference. Secondly, different models have different CPU, GPU, and memory requirements. Optimizing for one resource can lead to underutilization of other resources and higher costs. Amazon Web Services (AWS) resolves these cost-inefficient issues with **Amazon Elastic Inference**. You will learn more about Amazon Elastic Inference and when to use it in the next module.

### Problems

- Binary classification
- Multi-class classification
- Regression

### Classical programming vs. ML

- **Classical programming**: In classical programming, humans create rules based on factors like business requirements and domain knowledge. For instance, a programmer could set rules that say, "if this customer purchased product X in the past, show them product Y." The reason for this suggestion would be because of some relationship between those two products. Using a classical programming approach requires programmers to explicitly define and set rules.
- **ML**: Machine learning derives rules from the data itself. It uses a variety of data collected in the past to uncover patterns hidden in that data. The patterns are then used to create a model, which is applied to new data to provide a more well-informed and adaptive prediction. For instance, a programmer could build a model that collects data from each unique customer. Then based on the patterns the algorithm sees in the data, it can provide a unique recommendation for each user. When using machine learning, patterns are used to provide recommendations.

### When to use ML

- Classical programming example
	- _Rules can be coded_: problem can be solved with simple rules, computations, and predetermined steps without any data-driven learning
- ML
	- _Rules cannot be coded_: rules depend on many factors
	- _Scaling is not an option_: You might be able to manually recognize a few hundred emails and decide whether they are spam or not. However, this task becomes tedious for millions of emails. ML solutions are effective at handling large-scale problems.

### ML benefits

- Benefit both company and consumer
- Examples: spam detection, fraud detector, recommendations

### Algorithm types

- Supervised learning
- Unsupervised learning: ex. clustering
- Reinforcement learning: ex. AWS virtual racing car. It runs in a virtual track and need to learn when to steer and throttle

### Features and weights

- Feature: important parts of the dataset
- Weight: how important is the feature to the accuracy of the outcome

## Course: ML Pipeline

![](Media/45543.png)

### Problem formulation

- Translate business problem to ML problem
- Critical questions to ask:
	- How is this task done today?
	- How will the business measure success?
	- How will the solution be used?
	- Do similar solutions exist, which you might learn from?
	- What assumptions have been made?
	- Who are the domain experts?
- Framing the problem:
	- traditional or ML
	- supervised or unsupervised
	- is there data for training a supervised model
	- find the simplest solution
- Framework:
	- The problem: customer ending VISA cards because of frauds
	- The intended outcome: stop frauds
	- A quantitative goal: 10% reduction in frauds within a 6-month period
- Make it a ML model
	- The goal is a binary classification problem - fraud or not fraud

### Data collection

- Questions:
	- What data are needed?
	- Do I have access?
	- How much do I have? Where are they?
	- How to bring them to a central repository?
	- Is the data representative?
- Three types of data to consider:
	- **Private data** in various existing systems, like log files, customer invoices, etc.
	- **Commercial data** is data that a commercial entity collected and made available. Companies such as Reuters, Change Healthcare, Dun & Bradstreet, and Foursquare maintain databases that you can subscribe to. These databases include curated news stories, anonymized healthcare transactions, global business records, and location data. Supplementing your own data with commercial data can provide useful insights that you would not have otherwise.
	- **Public data**: This data comprises many different open-source datasets that range from scientific information to movie reviews. These datasets are usually available for use in research or for teaching purposes. You can find open-source datasets hosted by AWS, Kaggle, and the UC Irvine Machine Learning Repository. Government and health organizations are other sources of data that might be useful.
- Data considerations
	- Understand the data: ask yourself the questions above
	- Get a domain expert: use domain expertise to understand what data will be needed for making the best predictions
	- Evaluate data quality: does it include the important features for the prediction target?
	- Identify features and labels: what are being used to predict what?
	- Identify labeled data needs: what labels need to be created for the data. ex. for self-driving, where is the car, the obstacles, the distance between them, etc.
- Data preparation and preprocessing
	- When data are assembled from different sources, different feature values need to be harmonized
	- Sometimes requires subject matter to make correct preprocessing

### Feature engineering

- Selecting or creating the features that you will use to train your model
- ML model uses the features to train the model
- Feature extraction: reformatting, combining, and transforming primary features into new ones
	- Encoding ordinal and non-ordinal data
	- Finding missing data
	- Handling outliers in data
	- Rescaling data
- Feature selection: selecting features that are most relevant and discarding the rest. Prevents redundancy and overfitting
	- Filtering based: use statistical methods; fastest
	- Wrapper methods: training a model and measure the success of the model to get the usefulness of a subset of features; risk of overfitting
	- Embedded methods: algorithm specific and might use a combination of both
- Preparing data: eliminate dirty data to improve outcome
	- Encoding data: text to numeric, coding for ordinal and non-ordinal
	- Cleaning data: 
		- convert text to standard text
		- normalization to common scale
		- parse combined variable to multiple columns
		- delete rows or impute for missing data
	- Finding and handling missing data: most ML algorithms cannot deal with missing data
	- Handling outliers: sometimes need to be removed
		- univariate: a single variation for a single variable
		- multivariate: a variation of two or more variables

### Select and train model

- Training:
	- Train: for training
	- Validation: for fine-tuning training parameters
	- Test: for final testing 
- Over and underfitting:
	- Splitting the data to capture over and underfitting
	- Overfitting: performs well in training, bad in testing
	- Underfitting: performs bad in training
	- Balanced: similar error in training and testing
- AWS SageMaker has built in ML algorithms that are optimized for speed, scale, and accuracy

### Evaluating and tuning model

- Iterative process
- Evaluate: how well it works on new and future data
- Success metric: should be linked to the business metric that defines success. The two often have high correlation
- Tuning: modify data, features, and hyperparameters to achieve high accuracy in validation or test data

### Deploy model

- Can be hosted in SageMaker
- Deployment allows getting performance metrics from the model by making inferences on new data
- Deployment for testing vs. for production. Same mechanism, but different scales of requests

## Course: ML Tools

### ML stack layers

- A stack is a collection of resources that you can manage as a single unit. Thus, you can create, update, or delete a collection of resources by creating, updating, or deleting stacks
	- Data layer: where the data that will feed directly into your ML model is stored
	- Model layer: the model and algorithms that build predictions based on the data that the model collects
	- Deployment and monitoring layer: in this layer, the model is working in a live environment and producing ML tasks

### Tools for ML

- [Jupyter](Programming%20Environment/jupyter.md) notebook
- JupyterLab
- Pandas
- Matplotlib
- Seaborn
- NumPy
- Scikit-learn

### Machine learning framework

- Offers
	- Customized scripting
	- Community of developers
	- Integration with AWS
- ML framework tools:
	- PyTorch
	- Caffe2
	- Torch
	- TensorFlow
	- Gluon
	- Chainer
	- Keras
	- CNTK
	- Apache MXNet
- All supported and can be used from Amazon SageMaker

### Amazon instances for ML

> [!note] Amazon EC2 C5 and C5n Instances
> Amazon EC2 C5 instances deliver cost-effective high performance at a low price per compute ratio for running advanced compute-intensive workloads. For example, these workloads can include scientific modeling, distributed analytics, and machine learning or deep learning inference. The C5 instances help to speed up typical machine learning operations.

> [!note] Amazon EC2 P3 Instances
> E Amazon Elastic Compute Cloud (Amazon EC2) offers the P3 family of instances, which data scientists, researchers, and developers use to speed up ML applications. The Amazon EC2 P3 instances are the fastest in the cloud for ML training. They can reduce machine learning training time from days to minutes. The P3 instances are ideal for machine learning workloads that need massive parallel processing power.

> [!note] AWS IoT Greengrass
> AWS loT Greengrass makes it easy to bring intelligence to edge devices, such as for anomaly detection in precision agriculture or powering autonomous devices. AWS loT Greengrass provides an infrastructure for building machine learning for loT devices.

> [!note] Amazon Elastic Inference
> You can use Amazon Elastic Inference to attach low-cost GPU-powered acceleration to Amazon EC2 and SageMaker instances or Amazon Elastic Container Service (Amazon ECS) tasks. You can use Amazon Elastic Inference to choose the instance type that is best suited to the overall compute and memory needs of your application. You can then separately specify the amount of inference acceleration that you need. This process reduces inference costs by up to 75 percent because you no longer need to over-provision GPU compute for inference.

### ML services provided by AWS

- Computer vision
	- Amazon Rekognition: object and facial recognition for both image and video
	- Amazon Textract: extract text from images
- Chat interface
	- Amazon Lex: interactive conversational apps using voice or text
- Speech
	- Amazon Polly: converts text to speech
	- Amazon Transcribe: audio to text
- Fraud detecting
	- Amazon Fraud Detector: identify fraudulent transactions
- Language
	- Amazon Comprehend: find insights and relationships in text
	- Amazon Translate: translate text
- Recommendations
	- Amazon Personalize: create personalized recommendations for customers
- Amazon SageMaker: SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning models. It also provides the ability for developers to iterate the process until they get their model just right. The service quickly brings together a broad set of purpose-built capabilities for machine learning in one comprehensive service.
	- SageMaker brings all parts of [Course: ML Pipeline](#Course%20ML%20Pipeline) except problem formulation into one place

### SageMaker

#### SageMaker notebooks

Amazon SageMaker can deploy machine learning instances that run Jupyter notebooks and JupyterLabs. Amazon SageMaker manages the deployment of these compute resources, so you must connect to the Jupyter environment.

Jupyter notebooks are where you can create and share documents, code, and reports, and perform data visualizations in the same space. Notebooks also house frameworks, which are interfaces that provide pre-built and optimized components for conducting your ML work in the notebooks.

Notebooks, and the processing that they do, reside on virtual machines. In Amazon SageMaker, the virtual machines that you create are your fully managed Amazon SageMaker ML instances. Something that is fully managed on AWS means that AWS manages the creation of the instance and related resources so that you don't need to worry about it. Compute resources are fully elastic.

#### Instance types

With Amazon SageMaker, you can break out your instances by workload needs: notebook, training, and inferencing. (Inferencing refers to using the model to make predictions in production.)

![](Media/25465.png)

#### Data visualization

After you set up your notebook and get your data into it, look at that data visually.

Large amounts of data are easier for the brain to process visually. Fortunately, you can do this visualization from the Jupyter notebook in Amazon SageMaker.

Visualizing your data can help you select the features that you actually want to use when you build your model. Sifting through all the data to find patterns and associate meaning takes time, but to find a pattern is to find a solution. To help you find and see those patterns, you can build visualizations like histograms, cross-correlations of variables, and scatter plots.

#### Model selection

- A model can be made in SageMaker with:
	- SageMaker built-in algorithm
	- Write a script in your framework
	- Get an algorithm from AWS Marketplace
	- Bring your own algorithm
- After training your model, when you're ready to improve (or tune) it, Amazon SageMaker provides _automatic tuning_. This feature is also called hyperparameter tuning or hyperparameter optimization and is about adjusting and tweaking your algorithm.

#### Deployment

When iterating on your model and training it to be the best version it can be, you're ready to deploy it and start getting predictions. Amazon SageMaker provides several features to help manage resources and continue optimizing.

Using Amazon SageMaker, you can deploy a model by using a couple of different methods:
- ﻿﻿You can use the first way to get one prediction at a time.
- ﻿﻿You can use the second way to get predictions on an entire dataset.

#### Marketplace integration

Amazon SageMaker integrates with AWS Marketplace, which provides a large selection of ready-to-use model packages and algorithms from machine learning developers. These models and algorithms are ready to deploy in SageMaker.

Developers can use this integration to charge other SageMaker users for the use of their algorithms and model packages. The AWS Marketplace is a curated digital catalog that makes it easy for customers to find, buy, and deploy ML solutions for their business needs.

- Developers build models and algorithms in SageMaker then list them on the AWS Marketplace
- Businesses buy the prepackaged models and algorithms off the AWS Marketplace
- Developers make money and businesses save money

## Course: Wrapping Up

- ML ramp-up guide: https://d1.awsstatic.com/training-and-certification/ramp-up_guides/Ramp-Up_Guide_Machine_Learning.pdf; various AWS resources for ML training
- ML on AWS website: https://aws.amazon.com/ai/machine-learning/
- Blogs: https://aws.amazon.com/blogs/machine-learning/
- ML solutions in AWS Marketplace: https://aws.amazon.com/marketplace/solutions/machine-learning
- AWS ML services documentation: https://docs.aws.amazon.com/machine-learning/?id=docs_gateway
- ML in the AWS Partner Network: https://aws.amazon.com/partners/

