# Fundamentals of AI and ML

[0. AWS Introducing Generative AI](Machine%20Learning/AWS%20Introducing%20Generative%20AI/0.%20AWS%20Introducing%20Generative%20AI.md)

## Artificial Intelligence: Machine Learning

- You can: 
	- Identify your favorite song
	- Think of similar songs
	- Guess the artist with a certainty
- By processing multifaceted information
	- Sound
	- Lyrics
	- Artist
	- Video
	- Title, and
	- Similarity to other songs.
- This needs _information processing_ and _decision making_
- How to make computers do it? Machine learning

## What is ML

- A part of Artificial Intelligence
- A ML system can learn to play a game
- Parts
	- Data: images, text, pixels, numbers, any digital information
	- ML algorithms: rules on how to process the data
		- Model: how to learn from data
	- Training: use the data to train the model
	- Trained model: tested on new data
	- Iteration: the cycle of tweaking algorithms and data, coupled with re-training, aiming to uplift model performance.
- Many decisions to be made for each part

## Demo: Machine Learning

- Advertisement time!
- AWS provides pre-trained models directly for use
- Amazon Rekognition: computer vision model
	- face recognition
	- image label recognition: train, person, bracelet labels, location, and confidence score
- Integrate into apps via API calls
	- Send image as bytes
	- Receive outputs: confidence score, bounding box location

## Types of Machine Learning

- Supervised: data with labels
	- Learn mapping from inputs to outputs
	- When to use: classifying images, speech recognition, self driving cars, recognize songs and artists, detecting bank fraud
- Unsupervised: no label, find patterns on its own
	- When to use: creating new songs, generative learning to write code
	- All by finding the pattern in the data by the model itself
- Reinforced learning: takes actions and is rewarded or penalized; system learns behaviors that are successful
	- ex. learning to play a game
	- When to use: learn from interacting with the environment, robots learning to move, real time decision making for self driving car
- Real life condition require a combination of learning strategies

## Kinds of ML algorithms

- Traditional ML approaches
	- Decision trees: nodes represent aspects of data, branches represents rules, and leaves represent decisions
	- Regression: predicting values
	- Clustering: grouping data points based on similarity
- Deep learning
	- Neural network learns from data and adjusts weights
	- Applications:
		- customer churn: whether customer will leave
		- photo tagging ([CNN](Machine%20Learning/VU%20Deep%20Learning/3.%20VU%20DL%20Convolutional%20Network.md))
		- stock prediction ([RNN](Machine%20Learning/VU%20Deep%20Learning/4.%20VU%20DL%20Sequential%20Data.md))
	- Generative deep learning
		- Create new content
		- Compose new songs
		- Transformer architecture
			- Encoder-decoder structure
			- Translation
			- Generate code
- How to decide:
	- traditional: structured data
	- DL: large complex data with intricate pattern

## Decision tree example and interpretation

### Interpreting decision tree

- How a decision tree makes a decision can be visualized (see Jupyter notebook)
- The **Gini impurity** is a metric used to evaluate how often a randomly selected data point from a dataset would be misclassified if it were assigned a label based on the overall distribution of labels in that dataset. 
	- **Range**: The Gini impurity ranges from 0 to 0.5.
		- A value of 0 indicates perfect purity, meaning all data points in the subset belong to the same class. 
		- A value of 0.5 suggests that the data points are evenly distributed across different classes, indicating maximum impurity.
	- In decision trees, a lower Gini impurity is preferred because it signifies a node with more homogeneous data points, leading to more accurate splits and predictions.
- **Samples**
    - This value represents the number of samples (or records) that reach the node.
    - It gives an idea of how much of the training data is affected by the conditions leading to this node.
    - A high number of samples in a node means that the condition or rule associated with that node is relevant for a significant portion of the dataset.
- **Value**
    - This shows the distribution of the samples in different classes at that particular node.
    - For a binary classification problem (like churn prediction with 'Yes' or 'No'), the value is presented as a list of two numbers. The first number indicates the count of samples in the first class, and the second number indicates the count of samples in the second class.
    - This distribution helps in understanding which class is _predominant_ at a particular node.
- **Class**
    - This indicates the class that would be predicted if the decision tree traversal ends at that node.
    - It is determined based on the **majority class** of the samples that reach the node. For instance, if most samples at a node belong to the 'No Churn' class, the node will predict 'No Churn'.
- **Feature Name (e.g., 'Monthly Charge')**
    - This is not a standard part of the decision tree node description, but it may appear in the tree's branches.
    - It represents the feature (or attribute) used to split the data at that node.
    - For example, if you see "MonthlyCharge <= 80", it means that the tree is splitting the data at this node based on whether the monthly charge is less than or equal to 80.

### Key concepts

1. **Splitting the Dataset** - The dataset is divided into training and testing sets. Typically, 70% of the data is used for training the model, and the remaining 30% is reserved for testing.
2. **Training Data vs. Testing Data** - Training data is used to train the machine learning model. In contrast, testing data, which the model has not seen during training, is used to evaluate the model's performance and generalization ability.
3. **Model Training Process** - The process involves using a 'fit' method where the model is trained using features (X_train) and targets (Y_train). The testing data is not used in this stage.
4. **Prediction and Accuracy Assessment** - After training, the model makes predictions on the test data (X_test). These predictions are then compared with the actual outcomes (Y_test) to calculate the model's accuracy.

## Neural network example and interpretation

- Not as easily interpretable as a decision tree
- **Training Process** - You can plot the training and validation loss and accuracy over epochs to understand how the model is learning.
- **Decision Boundary** - For a simple neural network like the one in our example (with two input features), you can visualize the decision boundary on a 2D plot.

### Model accuracy and loss

In the context of the neural network exercise for predicting customer purchase behavior, "model accuracy" and "model loss" are two important metrics used to evaluate the performance of the model. Here's a concise explanation of each:

1. **Model Accuracy** is the fraction of predictions our model got right. In the context of the exercise, it is the proportion of correctly predicted purchase decisions (both purchases and non-purchases) out of all predictions made.
    - **Formula** - Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)
    - **Interpretation** - a higher accuracy indicates a better performing model. For example, an accuracy of 0.90 means that 90% of the model's predictions are correct.
2. **Model Loss** measures how far the model's predictions are from the actual class labels. It is a measure of the model's error.
    - **Binary Cross-Entropy** - is commonly used in binary classification tasks. It calculates the loss for each instance by comparing the predicted probability with the actual label (either 0 or 1), and then takes the average over all instances.
    - **Interpretation** - lower loss values are better, indicating that the model's predictions are closer to the actual labels. A high loss value means the model's predictions are far off from the actual labels.

### Decision boundary

In the context of the neural network exercise for predicting customer purchase behavior, the decision boundary is a concept used to visualize how the neural network categorizes or classifies the data.

The **Decision Boundary** is a boundary in the feature space that separates the data points into different classes based on the predictions made by the model. In our exercise, we have two input features: website visit duration and number of pages visited. The decision boundary will be a line or curve in this 2-dimensional space that separates the points predicted to make a purchase (class 1) from those predicted not to make a purchase (class 0).

- **How It's Determined** - The neural network learns the decision boundary during training. It adjusts its internal parameters (weights and biases) to correctly classify the training data. The boundary is where the model's output (before applying a threshold) is at some critical value, often 0.5 for binary classification with a sigmoid output. Points on one side of the boundary are classified as one class, and points on the other side as the other class.
- **Visualization** - In the plot, the decision boundary is typically represented by a line or contour. Points falling on one side of the boundary are predicted to be in one class, and those on the other side in the other class. The decision boundary can be linear or non-linear depending on the complexity of the model and the nature of the data.

The decision boundary in our neural network example serves as a visual tool to understand how the model differentiates between customers likely to make a purchase and those who are not, based on their website engagement metrics. It only shows what decisions are made but not _how_ the decisions are made.

## Discriminative and Generative AI

- **Discriminative**: identifying boundaries within the data
	- Ex: cat or dog
	- Typically supervised
	- Traditional: decision tree, linear regression
	- CNN: image recognition, song recognition
	- RNN: time series data, text
	- self-driving: identifying objects
	- identifying bank fraud
- **Generative**: generative new data similar to input
	- generate new songs
	- unsupervised, semi-supervised learning
	- ex. HMM can generate new data
	- ex. VAE, GAN: music, text
	- Transformer: NLP